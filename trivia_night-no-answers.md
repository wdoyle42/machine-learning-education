Trivia Night!
================
Will Doyle
2025-03-06

### **Easy Level**

1.  **What is the purpose of regularization in a GLM model?**

    1)  To make the model more complex  
    2)  To prevent overfitting by penalizing large coefficients  
    3)  To remove outliers from the dataset  
    4)  To improve accuracy by using all available features

2.  **Which tree-based model is an ensemble method that builds multiple
    decision trees and averages their predictions?**

3.  **Which type of validation method divides the dataset into multiple
    subsets, training the model on most of the subsets while testing on
    the remaining one, repeating this process multiple times so that
    each subset is used for testing exactly once?**

### **Medium Level**

4.  **What is the main advantage of XGBoost over traditional decision
    trees?**

    1)  It requires no hyperparameter tuning  
    2)  It uses boosting to iteratively improve weak models  
    3)  It only works on small datasets  
    4)  It does not use feature importance

5.  **Which of the following is a key assumption when using Support
    Vector Machines (SVMs) with a linear kernel?**

    1)  The data must be normally distributed  
    2)  The classes must be linearly separable  
    3)  There must be at least 1000 observations  
    4)  The features must be standardized

6.  **Which two hyperparameters are commonly tuned in elastic net
    regression using cross-validation?**

------------------------------------------------------------------------

### **Hard Level**

7.  **In Bayesian optimization for hyperparameter tuning, what role does
    the acquisition function play?**
    1)  It selects the next set of hyperparameters to evaluate  
    2)  It initializes the first batch of hyperparameters randomly  
    3)  It penalizes high-dimensional search spaces  
    4)  It directly minimizes the loss function
8.  **What is one potential drawback of using simulated annealing for
    hyperparameter tuning?**
    1)  It always converges to the same optimal solution  
    2)  It cannot escape local minima  
    3)  It requires another hyperparameterâ€“ the predefined schedule for
        temperature reduction  
    4)  It is only applicable to linear regression models
9.  **In glmnet, what happens to the number of nonzero coefficients as
    the lambda parameter increases? Does the number increase or
    decrease?**

Bonus question plus bet:

**Which learning method we discussed builds learners sequentially, where
each new learner corrects the errors of the previous ones?**
