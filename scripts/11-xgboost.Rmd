---
title: "Xgboost"
author: "Will Doyle"
date: "2025-02-18"
output: github_document
---

# Extreme Gradient Boosting (XGBoost) and Tree-Based Ensembles

XGBoost (Extreme Gradient Boosting) builds upon traditional gradient boosting by making the process faster, more efficient, and better at generalizing to new data. It introduces several innovations designed to overcome the weaknesses of standard gradient boosting:

Second-Order Optimization: Instead of just using the gradient of the loss function (which tells how much to adjust predictions), XGBoost also incorporates the second derivative (Hessian) to improve learning efficiency. This allows it to make better-informed adjustments at each step.

Feature Subsampling: Inspired by random forests, XGBoost randomly selects a subset of features at each split in a tree, reducing correlation between individual trees and improving overall model generalization.

Efficient Tree Construction: Traditional gradient boosting builds trees sequentially, evaluating many possible splits, which can be slow. XGBoost optimizes this process by using a technique called weighted quantile sketching, allowing it to handle very large datasets efficiently.

Handling Missing Data: Unlike many other models that require explicit imputation, XGBoost can learn the best direction to go when encountering missing values, improving robustness.

Together, these enhancements make XGBoost one of the most powerful machine learning techniques, widely used in applications ranging from finance to healthcare to competitive data science challenges. By combining gradient boosting’s sequential refinement of predictions with efficient, regularized tree-building techniques, XGBoost delivers high accuracy while maintaining computational efficiency.

### Weighted Quantile Sketching in XGBoost

**Understanding the Challenge:**
In traditional decision tree algorithms, finding the best split for a feature requires sorting all data points to identify threshold values. This process can be computationally expensive, especially for large datasets with continuous features. XGBoost addresses this challenge by using a method called **weighted quantile sketching**.

**What is Quantile Sketching?**
Quantile sketching is a technique used to approximate the distribution of data efficiently. Instead of sorting the entire dataset (which can be slow and memory-intensive), the algorithm estimates key quantiles—like medians or percentiles—that represent the data's distribution. This approximation is particularly useful for finding the best split points in decision trees.

**The “Weighted” Aspect:**
In XGBoost, data points can have different importance or weights, especially when correcting errors in boosting iterations. Weighted quantile sketching takes these weights into account, ensuring that the approximation reflects the actual influence of each data point. For example, data points with higher weights (often those with higher errors) will have more impact on determining the quantiles.

**How It Works in XGBoost:**
1. **Summarizing Data:** The algorithm summarizes feature values into a compact set of representative quantiles. This summary captures the distribution without needing to sort all data points.

2. **Approximate Splitting:** Instead of evaluating every possible threshold, XGBoost uses these quantiles to approximate the best splits quickly. This allows it to efficiently handle large datasets and high-cardinality features.

3. **Precision Control:** XGBoost ensures that the approximation is within a controllable error bound. This means it balances speed with accuracy, providing high-quality splits without excessive computational cost.

**Benefits of Weighted Quantile Sketching:**
- **Scalability:** It enables XGBoost to handle massive datasets with millions of data points and features.
- **Efficiency:** Reduces the time complexity of finding optimal splits, especially for continuous variables.
- **Robustness:** Accurately reflects the distribution of data, even when some points have higher significance due to their errors.

**Practical Impact:**
This innovation is one of the reasons why XGBoost is so fast and reliable in real-world applications. It combines the power of gradient boosting with clever statistical approximations to deliver state-of-the-art performance without compromising computational efficiency.

### **Stochastic Gradient Boosting**  

Stochastic Gradient Boosting (SGB) is a variation of traditional gradient boosting that introduces **randomness** to improve generalization and efficiency. Instead of using the full dataset for every boosting iteration, SGB **randomly samples a subset of the training data** at each step. This controlled randomness helps prevent overfitting and makes the model more robust to noise.

#### **How It Works**
1. **Sequential Learning:** Like standard gradient boosting, SGB builds models iteratively, with each new tree focusing on correcting the errors of the previous ones.
2. **Row Subsampling:** Instead of training each tree on the entire dataset, SGB **randomly selects a fraction of the available data** (e.g., 70%) at each iteration. This means different trees see slightly different views of the data.
3. **Feature Subsampling (Optional):** Some implementations, including XGBoost, also allow **feature subsampling**, where a random subset of features is chosen for each tree or each split.
4. **Gradient-Based Updates:** The algorithm still follows gradient descent principles, updating the model based on residual errors, but each update is based on a **randomized subset of the data**, making it more resistant to overfitting.

#### **Key Benefits of Stochastic Gradient Boosting**
- **Prevents Overfitting:** By training on random subsets of data, SGB reduces the model’s tendency to memorize specific data points.
- **Speeds Up Training:** Since each tree only trains on a portion of the data, computation is faster.
- **Increases Model Diversity:** The randomness makes individual trees more different from each other, leading to better generalization.

#### **Stochastic Gradient Boosting vs. Standard Gradient Boosting**
- **Standard Gradient Boosting:** Uses all available data at each boosting step, making it more prone to overfitting.
- **Stochastic Gradient Boosting:** Introduces randomness by sampling data, improving generalization and efficiency.

This approach is particularly effective in **large datasets** and **high-variance settings**, making it a core component of algorithms like **XGBoost**, which further optimizes it with additional enhancements like regularization and second-order gradient approximations.

```{r}
library(tidyverse)
library(tidymodels)
library(janitor)
library(xgboost)
library(vip)
library(rpart)
library(rpart.plot)
```

```{r}
ou<-read_csv("oulad.csv")%>%
  mutate(result=fct_relevel(as_factor(result),c("passed","not_passed")))%>%
  select(-final_result)
```

```{r}
ou_split<-initial_split(ou)

ou_train<-training(ou_split)

ou_test<-testing(ou_split)
```


## Formula and Recipe, same as last time

```{r}
rf_formula<-as.formula("result~.")
```


```{r}
ou_rec<-recipe(rf_formula,ou_train)%>%
  update_role(result,new_role = "outcome")%>%
  step_other(all_nominal_predictors(),threshold = .05)%>%
  step_unknown(all_nominal_predictors())%>%
  step_dummy(all_nominal_predictors())%>%
  step_zv(all_predictors())
```




## XGboost Specification

From: https://juliasilge.com/blog/xgboost-tune-volleyball/


```{r}
xgb_spec <- boost_tree(
  trees = 100, 
  tree_depth = tune(), 
  min_n = tune(),
  loss_reduction = tune(), ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),         ## randomness
  learn_rate = tune() ## step size
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")
```

# XGBoost Hyperparameter Descriptions

## 1. trees (Number of Boosted Trees)
- **Definition**: The total number of boosting iterations (i.e., the number of trees in the ensemble).
- **Effect**: More trees generally lead to better performance, but excessive trees can result in overfitting.
- **Default in XGBoost**: 100
- **Tuning Considerations**: Usually, you should tune this in combination with `learning_rate`—lower learning rates often require more trees.

## 2. tree_depth (Maximum Depth of a Tree)
- **Definition**: The maximum depth each individual decision tree can reach.
- **Effect**:
  - Deeper trees capture more complex interactions but can overfit.
  - Shallower trees promote regularization and generalization.
- **Default in XGBoost**: 6
- **Typical Range**: 3–10
- **Tuning Considerations**: Start with a small value (e.g., 3–6) and increase gradually. Higher values require more computational power.

## 3. min_n (Minimum Node Size)
- **Definition**: The minimum number of observations required in a node to allow further splits.
- **Effect**:
  - Higher values prevent overfitting by ensuring splits happen only when there's sufficient data.
  - Lower values make the model more flexible but increase the risk of overfitting.
- **Default in XGBoost**: 1
- **Typical Range**: 1–20
- **Tuning Considerations**: Start with a lower value and increase if overfitting occurs.

## 4. loss_reduction (Gamma)
- **Definition**: The minimum loss reduction required to make a further split in a tree.
- **Effect**:
  - Higher values prevent splits that don't offer significant information gain, acting as a form of regularization.
  - Lower values allow more splits, making the model more flexible.
- **Default in XGBoost**: 0
- **Typical Range**: 0–10
- **Tuning Considerations**: If overfitting occurs, try increasing this value.

## 5. sample_size (Subsample Ratio)
- **Definition**: The fraction of training data used to grow each tree.
- **Effect**:
  - Values < 1 introduce randomness, improving generalization and reducing overfitting.
  - Values closer to 1 lead to deterministic trees with less variance but more risk of overfitting.
- **Default in XGBoost**: 1 (no subsampling)
- **Typical Range**: 0.5–1
- **Tuning Considerations**: Lower values (e.g., 0.7–0.9) often work well.

## 6. mtry (Feature Subsampling)
- **Definition**: The number of features randomly selected at each split.
- **Effect**:
  - Lower values introduce more randomness, reducing overfitting.
  - Higher values make the model more deterministic.
- **Default in XGBoost**: Uses all features
- **Typical Range**: `sqrt(number of features)` or `log2(number of features)`
- **Tuning Considerations**: Start with `sqrt(p)` for classification and `p/3` for regression.

## 7. learn_rate (Learning Rate)
- **Definition**: The step size shrinkage parameter that controls how much the model learns from each tree.
- **Effect**:
  - Lower values make learning more gradual, reducing the risk of overfitting.
  - Higher values speed up training but risk missing the optimal solution.
- **Default in XGBoost**: 0.3
- **Typical Range**: 0.01–0.3
- **Tuning Considerations**: Lower values (e.g., 0.01–0.1) often work better, but may require more trees.


## Summary of Tuning Considerations

| Hyperparameter | Regularization Effect | Flexibility |
|---------------|----------------------|------------|
| `tree_depth` | Higher depth → More complexity, more overfitting | Shallower trees generalize better |
| `min_n` | Higher values prevent splits with few data points | Lower values allow more splits |
| `loss_reduction` | Higher values prevent unnecessary splits | Lower values allow more granular splits |
| `sample_size` | Lower values introduce more randomness | Higher values make trees more deterministic |
| `mtry` | Lower values introduce randomness | Higher values use more features per split |
| `learn_rate` | Lower values improve generalization | Higher values speed up training |




```{r}
xgb_grid <- grid_space_filling(
  tree_depth(),
  min_n(),
  loss_reduction(), 
  sample_size = sample_prop(),
  finalize(mtry(), ou_train),
  learn_rate(),
  size = 30
)
```


```{r}
ou_wf <- workflow() %>%
  add_recipe(ou_rec) %>%
  add_model(xgb_spec)
```

```{r}
ou_rs<-ou%>%vfold_cv()
```

## Fit Model

```{r}
fit_model<-FALSE

if(fit_model){
xg_tune_res <- tune_grid(
  ou_wf,
  grid=xgb_grid,
  resamples = ou_rs,
)
save(xg_tune_res,file="xg_tune_res.Rdata")
} else{
  load("xg_tune_res.Rdata")
}
```

```{r}
xg_tune_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")
```

```{r}
show_best(xg_tune_res,metric="roc_auc")
```

```{r}
best_auc <- select_best(xg_tune_res,metric =  "roc_auc")
```

```{r}
final_xgb <- finalize_workflow(
  ou_wf,
  best_auc
)
```

## Variable importance
How xgb.importance() Computes and Scales Feature Importance

Feature importance in XGBoost is derived from individual trees and aggregated across all boosting rounds. The scores are then normalized so that they sum to 1 (or 100%) for easier interpretation.

**1. Raw Computation (Before Scaling)**

For each feature, XGBoost tracks:

Gain: The total improvement in loss function when a feature is used in a split.

Cover: The total number of samples impacted by splits on a feature.

Frequency: The number of times a feature is used for splitting.

At first, these values are raw counts or sums.

**2. Normalization (Scaling)**

By default, xgb.importance() scales the importance scores so they sum to 1 (or 100%):

Normalized Importance = (Raw Importance for Feature i) / (Sum of Raw Importance for All Features)

Thus, after scaling:

The sum of all feature importance values is 1 (or 100%).

Each feature's importance is expressed as a relative proportion.

```{r}
final_xgb %>%
  fit(data = ou_train) %>%
  extract_fit_parsnip() %>%
  vip(geom = "point")+
  geom_point(color="blue")+
  theme_minimal()
```

```{r}
final_res <- last_fit(final_xgb, ou_split)

collect_metrics(final_res)
```

More intuitive explanation of the gradient descent step, how it relates to the Hessian. 

Below, we're going to build a stochastic gradient descent model by hand. This isn't a full xgboost, but gives you a sense of how the procedure works. To keep things simple, we're going to work with a sample of just 100 units from the overall dataset. 

```{r}

# Sample 100 units from dataset `ou`
sample_data <- ou%>%
  sample_n(100)%>%
  mutate(y=ifelse(result=="passed",1,0))%>%
  select(where(is.numeric))
```

We'll next define a few parameters, including the learning rate, the sampling rate, and the number of features to try. 

```{r}
# Define learning rate
eta <- 0.1

# Define sampling rate

sampling_rate<-.8

# Define number of boosting iterations
num_iterations <- 100

# Number of features to randomly select in each iteration
num_features <- 5

```

The procedure starts by just guessing the mean as the prediction.

```{r}
# Initialize predictions as the mean of y
sample_data$prediction <- mean(sample_data$y)

 # Compute residuals (errors)
  sample_data$residuals <- sample_data$y - sample_data$prediction
```


I'm going to store the results as I go
```{r}
# Store predictions over iterations for visualization
iteration_results <- data.frame(ID = 1:nrow(sample_data), y_true = sample_data$y)

  
```

Below is the boosting loop. It works like this:

1. Sample 80% of the data
2. Randomly select 5 features
3. Calculate the residuals from the existing predictions
4. Run a tree using the sampled data and 5 randomly selected features
5. Use that tree to predict the probability of the outcome in the FULL dataset
6. Use the prediction times the learning rate eta as the prediction for that iteration

6. Iterate over steps 1-

```{r}
# Boosting loop
for (i in 1:num_iterations) {
  cat("\nIteration:", i, "\n")
  
  # Sample 80% of data
    sampled_data <- sample_frac(sample_data,sampling_rate)
  
  # Select a random subset of features (excluding target and prediction columns)
  feature_cols <- setdiff(names(sampled_data), c("y", "prediction"))
  selected_features <- sample(feature_cols, num_features)
  
 
  # Fit a regression tree using only selected features
  formula <- as.formula(paste("residuals ~", paste(selected_features, collapse = " + ")))
  
  tree <- rpart(formula, data = sampled_data, method = "anova")
  
  
#dev.new()
  
#title=paste("Results for iteration",i)
#rpart.plot(tree,main=title)

  # Get tree predictions (fitted residuals) for the full dataset (sample_data)
  sample_data$tree_output <- predict(tree, sample_data)
  
  ## What are predictions?
  

  # Update predictions using learning rate
  sample_data$prediction <- sample_data$prediction + eta * sample_data$tree_output
  
  # Compute new residuals
  sample_data$residuals <- sample_data$y - sample_data$prediction
  
  # Store predictions for visualization
  iteration_results[[paste0("Iteration_", i)]] <- sample_data$prediction
}


# Convert data for ggplot visualization
plot_data <- iteration_results %>%
  pivot_longer(cols = starts_with("Iteration"), names_to = "Iteration", values_to = "Prediction")

# Final plot showing prediction convergence
ggplot(plot_data, aes(x = ID, y = Prediction)) +
  geom_point(size = .1,alpha=.3) +
  geom_line(aes(group = Iteration), alpha = 0.7) +
  geom_point(aes(y = y_true), shape = 4, color = "black", size = 3) +  # True values
  labs(title = "Prediction Convergence Over Iterations",
       x = "Observation ID",
       y = "Predicted Value") +
  theme_minimal()+
  theme(legend.position="none")



```

```{r}
results_tbl <- sample_data %>%
    mutate(y = as.factor(y)) %>%
    select(y, prediction)
  
  # Compute ROC AUC using yardstick
  auc_score <- roc_auc(results_tbl, truth = y,prediction,event_level = "second")
```


```{r}
graphics.off()
```

