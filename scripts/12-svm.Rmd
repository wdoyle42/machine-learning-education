---
title: "Support Vector Machines"
author: "Will Doyle"
date: "2025-02-25"
output: github_document
---
 
### **Support Vector Machines (SVM)**

#### **1. Introduction**
Support Vector Machines (SVMs) are a powerful class of supervised learning models used for classification and regression tasks. They are particularly effective in high-dimensional spaces. SVMs work by finding the optimal hyperplane that best separates classes in a dataset.

---

### **2. Key Concept: The Optimal Hyperplane**
For a **binary classification** problem, SVM aims to find the **decision boundary** (hyperplane) that maximizes the margin between two classes.

- The **hyperplane** is defined as:
  \[
  w \cdot x + b = 0
  \]
  where:
  - \( w \) is the **weight vector** (direction of separation),
  - \( x \) represents the **feature values**,
  - \( b \) is the **bias term** (intercept).

- The margin is the distance between the hyperplane and the nearest data points (support vectors).
- The larger the margin, the better the generalization of the model.

#### **Finding the Best Hyperplane**
SVM optimizes the margin by solving:
\[
\min_{w, b} \quad \frac{1}{2} ||w||^2
\]
subject to the constraint:
\[
y_i (w \cdot x_i + b) \geq 1, \quad \forall i
\]
where \( y_i \) is the class label (\(+1\) or \(-1\)).

---

### **3. Support Vectors: The Most Informative Points**
Support vectors are the **critical data points** that define the margin. These are:
- **Points on the margin boundaries**.
- **Misclassified points (in soft-margin SVMs)**.

SVM ignores **non-critical points** in training, which makes it robust to outliers and high-dimensional data. This last point means that it will many times reduce the effective sample used for estimation by quite a bit. 

---

### **4. Hard-Margin vs. Soft-Margin SVM**
#### **Hard-Margin SVM (No Overlapping Classes)**
- Assumes that data is **perfectly separable**.
- The margin is strictly enforced, allowing **no misclassifications**.
- Solves:
  \[
  \min_{w, b} \quad \frac{1}{2} ||w||^2
  \]
  subject to \( y_i (w \cdot x_i + b) \geq 1 \).

#### **Soft-Margin SVM (Overlapping Classes)**
- Allows for some misclassifications using **slack variables** \( \xi_i \).
- Trades off **margin width** and **classification errors** via **cost parameter \( C \)**.
- Solves:
  \[
  \min_{w, b} \quad \frac{1}{2} ||w||^2 + C \sum_{i=1}^{N} \xi_i
  \]
  where \( \xi_i \geq 0 \) allows points to be inside the margin or misclassified.

---

### **5. Nonlinear SVM: The Kernel Trick**
Many real-world datasets are not linearly separable. SVMs handle this using the kernel trick, which maps the input space into a higher-dimensional space where the data is separable.

**Common kernel functions**:
- **Linear kernel**:
  \[
  K(x_i, x_j) = x_i \cdot x_j
  \]
  Used for linearly separable data.

- **Polynomial kernel**:
  \[
  K(x_i, x_j) = (x_i \cdot x_j + c)^d
  \]
  Used for curved decision boundaries.

- **Radial Basis Function (RBF) kernel**:
  \[
  K(x_i, x_j) = \exp\left(-\gamma ||x_i - x_j||^2 \right)
  \]
  Handles complex decision boundaries by mapping data into an n-dimensional space.


RBF is flexible enough that it's used unless the function is kind of obviously linear. 

### **7. Hyperparameters in SVM**
- **Cost parameter \( C \)**:
  - **High \( C \)** : Small margin, fewer misclassifications (risk of overfitting).
  - **Low \( C \)** : Large margin, allows some misclassifications (better generalization).
  
- **Kernel-specific parameters**:
  - **RBF kernel (\(\gamma\))**:
    - **High \(\gamma\)** → More flexible decision boundary (risk of overfitting).
    - **Low \(\gamma\)** → Simpler decision boundary (risk of underfitting).
- **Polynomial Kernel**
  -**Value of the polynomial expansion
---

## Example 1: Linear Response function

```{r}
library(tidyverse)
library(tidymodels)
library(e1071)
library(kernlab)
```

```{r}

# Generate synthetic data

x <- seq(-3, 3, length.out = 100)   # Single feature
y <- ifelse(x + rnorm(100, sd=1) > 0, 1, 0)  # Binary target with some noise
data <- data.frame(x = x, y = as.factor(y))  # Convert y to a factor for classification

# Train SVM model
svm_model <- svm(y ~ x, data = data, kernel = "linear", cost = 1,scale=FALSE)

# Get the decision boundary
w <- t(svm_model$coefs) %*% svm_model$SV  # Compute weight
b <- -svm_model$rho  # Intercept

# Compute the decision boundary
decision_boundary_x <- -b / w  # Since w*x + b = 0, solve for x

# Predict using the trained model
data$predicted <- predict(svm_model, newdata = data)

# Visualize the decision boundary
ggplot(data, aes(x = x, y = as.numeric(y), color = predicted)) +
  geom_point(size = 3) +
  geom_vline(xintercept = decision_boundary_x, linetype = "dashed", color = "black") +
  labs(title = "SVM Classification with One Feature",
       x = "Feature x", y = "Class (y)") +
  theme_minimal()

## Another look
included<-(x%in%svm_model$SV)

df<-data.frame(x,y,included)

ggplot(df,aes(x=x,y=y,color=included))+
  geom_point()
```


## Example 2: Non-Linear Response function

```{r}

# Generate synthetic data where P(y=1) follows a sigmoid function of x

x <- seq(-3, 3, length.out = 100)  # Feature
beta <- 3  # Controls steepness of sigmoid
prob_y1 <- 1 / (1 + exp(-beta * x))  # Sigmoid function
y <- rbinom(100, 1, prob_y1)  # Sample binary outcomes with probabilities
data <- data.frame(x = x, y = as.factor(y))  # Convert y to factor for classification

data%>%
  ggplot(aes(x=x,y=y))+geom_point()

data%>%
  ggplot(aes(x=x,y=y))+geom_point()

# Train SVM with an RBF kernel
svm_model <- svm(y ~ x, data = data, kernel = "radial", cost = 1, gamma = 1)

# Extract Support Vectors
support_vectors_scaled <- svm_model$SV  # These are in scaled form

# Reverse scaling to get original x values
scaled_x <- scale(data$x)  # Get scaling parameters
original_support_vectors <- support_vectors_scaled * attr(scaled_x, "scaled:scale") + attr(scaled_x, "scaled:center")

# Predict decision values and apply sigmoid transformation
decision_values <- attributes(predict(svm_model, newdata = data, decision.values = TRUE))$decision.values
sigmoid_response <- 1 / (1 + exp(-decision_values))  # Approximate probability output

# Add predictions to the dataset
data$probability <- sigmoid_response
data$predicted <- predict(svm_model, newdata = data)

# Visualization: Decision Boundary & Support Vectors
ggplot(data, aes(x = x, y = as.numeric(y), color = probability)) +
  geom_point(size = 3) +
  geom_vline(xintercept = original_support_vectors, linetype = "dashed", color = "red") + 
  scale_color_gradient(low = "blue", high = "red") + 
  labs(title = "SVM with RBF Kernel on Sigmoid-Generated Data",
       x = "Feature x", y = "Class (y)") +
  theme_minimal()
```

## Application to Open University Data

```{r}

ou<-read_csv("oulad.csv")%>%
  mutate(result=fct_relevel(as_factor(result),c("passed","not_passed")))%>%
  select(-final_result,-repeatactivity)%>%
  sample_n(10000)

```

```{r}

ou_split<-initial_split(ou)

ou_train<-training(ou_split)

ou_test<-testing(ou_split)
```

```{r}
rf_formula<-as.formula("result~.")
```

```{r}
ou_rec<-recipe(rf_formula,ou_train)%>%
  update_role(id_student,new_role="id")%>%
  update_role(result,new_role = "outcome")%>%
  step_unknown(all_nominal_predictors())%>%
  step_other(all_nominal_predictors(),threshold = .05)%>%
  step_dummy(all_nominal_predictors())%>%
  step_zv(all_predictors())%>%
  step_normalize(all_predictors())%>%
  step_impute_mean(all_predictors())
```

```{r}
svm_model<-svm_rbf(
  mode="classification",
  cost=tune(),
  rbf_sigma=tune())%>%
  set_engine("kernlab")
```
```{r}
ou_rs <- mc_cv(ou_train, times=10)
```
```{r}
ou_grid<-grid_space_filling(extract_parameter_set_dials(svm_model),size=4)
```

```{r}
ou_wf <- workflow() %>%
  add_model(svm_model) %>%
  add_recipe(ou_rec)
```

```{r}
fit_svm=TRUE

if(fit_svm){
  doParallel::registerDoParallel()
  
ou_tune_results <- tune_grid(
  ou_wf,
  resamples = ou_rs,
  grid = ou_grid,
  metrics = metric_set(accuracy, roc_auc)
)
save(ou_tune_results,file="ou_tune_results.Rdata")
} else {
  load("ou_tune_results.Rdata")
}
```

```{r}
autoplot(ou_tune_results)
```

```{r}
best_params <- select_best(ou_tune_results, metric = "roc_auc")
print(best_params)
```


```{r}
final_ou_wf <- finalize_workflow(ou_wf, best_params)
```

```{r}
final_ou_model <- fit(final_ou_wf, data = ou_train)
```

```{r}
# Evaluate on test set
ou_test<-final_ou_model%>%augment(ou_test)

# Compute final accuracy and ROC AUC
final_metrics <- ou_test %>%
  roc_auc(truth = result,.pred_passed) %>%
  bind_rows(
    ou_test %>% accuracy(truth = result, estimate = .pred_class)
  )

# Output results
print(final_metrics)
```


## Functional Changes and the Kernel Trick
```{r}

# Generate 6 points - three from each class
x1 <- matrix(c(1, 2, 1.5, 6, 7, 6.5), ncol = 1)  # Feature (1D)
y  <- factor(c(-1, -1, -1, 1, 1, 1))  # Class labels

df <- data.frame(x1, y)

# Plot points
ggplot(df, aes(x = x1, y = as.numeric(y), color = y)) +
  geom_point(size = 4) +
  labs(title = "Small Binary Classification Dataset", x = "Feature", y = "Class") +
  theme_minimal()

```

```{r}
# Train SVM with a linear kernel
svm_linear <- svm(y ~ ., data = df, kernel = "linear", cost = 10)

# Extract support vectors
sv_linear_sv <- df[svm_linear$index, ]

# Print support vectors
print(sv_linear_sv)

# Visualize support vectors
ggplot(df, aes(x = x1, y = as.numeric(y), color = y)) +
  geom_point(size = 4) +
  geom_point(data = sv_linear_sv, aes(x = x1, y = as.numeric(y)), 
             color = "black", shape = 1, size = 6, stroke = 2) +
  labs(title = "Linear Kernel: Support Vectors Highlighted", x = "Feature", y = "Class") +
  theme_minimal()
```

```{r}
# Train SVM with a polynomial kernel (degree = 2)
svm_poly <- svm(y ~ ., data = df, kernel = "polynomial", degree = 2, cost = 10)

# Extract support vectors
sv_poly_sv <- df[svm_poly$index, ]

# Print support vectors
print(sv_poly_sv)

# Visualize support vectors
ggplot(df, aes(x = x1, y = as.numeric(y), color = y)) +
  geom_point(size = 4) +
  geom_point(data = sv_poly_sv, aes(x = x1, y = as.numeric(y)), 
             color = "black", shape = 1, size = 6, stroke = 2) +
  labs(title = "Polynomial Kernel (Degree = 2): Support Vectors Highlighted", x = "Feature", y = "Class") +
  theme_minimal()
```

```{r}
# Define the RBF kernel function
rbf_kernel <- function(x1, x2, gamma = 0.5) {
  exp(-gamma * (x1 - x2)^2)
}

# Compute pairwise RBF kernel values
gamma <- 0.5
kernel_matrix <- outer(x1, x1, Vectorize(function(a, b) rbf_kernel(a, b, gamma)))

# Display kernel matrix
print(kernel_matrix)
```

```{r}
# Train SVM with RBF kernel
svm_model <- svm(y ~ ., data = df, kernel = "radial", gamma = 0.5, cost = 10)

# Extract support vectors
support_vectors <- df[svm_model$index, ]

# Print support vectors
print(support_vectors)

# Visualize support vectors
ggplot(df, aes(x = x1, y = as.numeric(y), color = y)) +
  geom_point(size = 4) +
  geom_point(data = support_vectors, aes(x = x1, y = as.numeric(y)), 
             color = "black", shape = 1, size = 6, stroke = 2) +
  labs(title = "Support Vectors Highlighted", x = "Feature", y = "Class") +
  theme_minimal()
```



```{r}
# Choose a test point to compare influence
test_x <- 5

# Compute kernel similarities with the test point
influence_values <- sapply(x1, function(x) rbf_kernel(x, test_x, gamma))

# Create dataframe for plotting
influence_df <- data.frame(x1 = x1, influence = influence_values)

# Plot influence of each point
ggplot(influence_df, aes(x = x1, y = influence)) +
  geom_point(size = 4, color = "blue") +
  geom_line() +
  labs(title = "RBF Kernel Influence on Test Point x=5",
       x = "Training Points", y = "Kernel Similarity") +
  theme_minimal()
```

### **How Similarity is Used to Predict Points in the Testing Dataset**

In **Support Vector Machines (SVMs)**, the **kernel similarity** between a new test point and training points is crucial for making predictions. 

---

## **1. Prediction in a Linear SVM**
In a **linear kernel**, the decision boundary is a straight line (or hyperplane in higher dimensions), and prediction is based on the **dot product**:

\[
f(x) = \sum_{i \in SV} \alpha_i y_i (x_i \cdot x) + b
\]

where:
- \( x \) is the test point,
- \( x_i \) are the **support vectors** (not all training points, only the critical ones),
- \( y_i \) are the labels of support vectors (+1 or -1),
- \( \alpha_i \) are learned coefficients (Lagrange multipliers),
- \( b \) is the bias term.

**Key Idea**: Prediction is a weighted sum of **similarity** (dot product) between the test point and support vectors.

---

## **2. Prediction in a Polynomial SVM**
The **polynomial kernel** is defined as:

\[
K(x_i, x_j) = (x_i \cdot x_j + c)^d
\]

where:
- \( c \) is a constant (we use **1**),
- \( d \) is the degree of the polynomial (we use **2** for quadratic separation).

#### **Impact on Prediction:**
- If a test point has **high polynomial similarity** to positive-class support vectors, it is classified as **+1**.
- If a test point has **higher similarity** to negative-class support vectors, it is classified as **-1**.
- **Non-linear decision boundaries** emerge because polynomial similarity is not just based on distance but **higher-order relationships**.

---

## **3. Prediction in an RBF (Gaussian) SVM**
The **RBF kernel** introduces a different notion of similarity:

\[
K(x_i, x_j) = \exp(-\gamma ||x_i - x_j||^2)
\]

where \( \gamma \) controls how **quickly similarity decreases** with distance.

#### **Impact on Prediction:**
- If a test point is **very close** to a particular support vector, the kernel similarity will be **near 1**.
- If it is **far away**, the kernel similarity **drops towards 0**.
- A **weighted sum** of these similarities determines classification:

\[
f(x) = \sum_{i \in SV} \alpha_i y_i K(x_i, x) + b
\]

---

## **4. How Kernel Influence Affects Classification**
To visualize how similarity determines classification in an SVM, let's compute the **weighted similarity scores** for a test point using R:


#### **Expected Observations**
- **Linear**: Influence increases linearly with \( x_1 \).
- **Polynomial**: Influence rises sharply for large \( x_1 \), showing **non-linear growth**.
- **RBF**: Influence **peaks near close points** and drops off exponentially.

This explains **why RBF SVMs perform well on non-linearly separable data**—they assign influence locally, avoiding the need for a complex decision boundary in original space.


```{r}

# Define a test point
test_x <- 5

# Define polynomial kernel function
poly_kernel <- function(x1, x2, degree = 2, c = 1) {
  (x1 * x2 + c)^degree
}

# Compute kernel similarities for each approach
gamma <- 0.5
influence_df <- data.frame(
  x1 = x1,
  Linear = x1 * test_x,
  Polynomial = sapply(x1, function(x) poly_kernel(x, test_x, degree = 2)),
  RBF = sapply(x1, function(x) exp(-gamma * (x - test_x)^2))
)

# Reshape data for visualization

influence_df_long <- influence_df%>%
  pivot_longer(cols=c("Linear","Polynomial","RBF"))

# Plot kernel influence
ggplot(influence_df_long, aes(x = x1, y = value, color = name)) +
  geom_point(size = 4) +
  geom_line() +
  labs(title = "Influence of Training Points (Different Kernels)",
       x = "Training Points", y = "Kernel Similarity") +
  theme_minimal()
```
