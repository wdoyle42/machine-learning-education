---
title: "Support Vector Machines"
author: "Will Doyle"
date: "2025-02-25"
output: github_document
---

### **Support Vector Machines (SVM)**

#### **1. Introduction**

Support Vector Machines (SVMs) are a powerful class of supervised learning models used for classification and regression tasks. They are particularly effective in high-dimensional spaces. SVMs work by finding the optimal hyperplane that best separates classes in a dataset.

------------------------------------------------------------------------

### **2. Key Concept: The Optimal Hyperplane**

For a **binary classification** problem, SVM aims to find the **decision boundary** (hyperplane) that maximizes the margin between two classes.

-   The **hyperplane** is defined as: $$
    w \cdot x + b = 0
    $$ where:
    -   $w$ is the **weight vector** (direction of separation),
    -   $x$ represents the **feature values**,
    -   $b$ is the **bias term** (intercept).
-   The margin is the distance between the hyperplane and the nearest data points (support vectors).
-   The larger the margin, the better the generalization of the model.

#### **Finding the Best Hyperplane**

SVM optimizes the margin by solving: $$
\min_{w, b} \quad \frac{1}{2} ||w||^2
$$ subject to the constraint: $$
y_i (w \cdot x_i + b) \geq 1, \quad \forall i
$$ where $y_i$ is the class label ($+1$ or $-1$).

------------------------------------------------------------------------

### **3. Support Vectors: The Most Informative Points**

Support vectors are the critical data points that lie closest to the decision boundary and directly influence its position—these are the points that sit exactly on the margin boundaries in a perfectly separable case, or those that violate the margin (including misclassified points) when using a soft-margin SVM that tolerates some error. What makes SVMs particularly elegant is that they ignore all other data points during training, focusing exclusively on these support vectors to define the decision boundary. This selective attention makes SVMs remarkably robust to outliers and effective in high-dimensional spaces, since the algorithm isn't swayed by the bulk of the data that lies far from the boundary. However, this also means that in practice, the effective sample size used for estimation can be dramatically reduced—sometimes only a small fraction of your original dataset actually influences the final model, which can be both a computational advantage and a potential concern depending on your data distribution.

### **4. Hard-Margin vs. Soft-Margin SVM**

#### **Hard-Margin SVM (No Overlapping Classes)**

-   Assumes that data is **perfectly separable**.
-   The margin is strictly enforced, allowing **no misclassifications**.
-   Solves: $$
    \min_{w, b} \quad \frac{1}{2} ||w||^2
    $$ subject to $y_i (w \cdot x_i + b) \geq 1$.

#### **Soft-Margin SVM (Overlapping Classes)**

-   Allows for some misclassifications using **slack variables** $\xi_i$.
-   Trades off **margin width** and **classification errors** via **cost parameter** $C$.
-   Solves: $$
    \min_{w, b} \quad \frac{1}{2} ||w||^2 + C \sum_{i=1}^{N} \xi_i
    $$ where $\xi_i \geq 0$ allows points to be inside the margin or misclassified.

------------------------------------------------------------------------

### **5. Nonlinear SVM: The Kernel Trick**

Many real-world datasets are not linearly separable. SVMs handle this using the kernel trick, which maps the input space into a higher-dimensional space where the data is separable.

**Common kernel functions**: - **Linear kernel**: $$
  K(x_i, x_j) = x_i \cdot x_j
  $$ Used for linearly separable data.

-   **Polynomial kernel**: $$
    K(x_i, x_j) = (x_i \cdot x_j + c)^d
    $$ Used for curved decision boundaries.

-   **Radial Basis Function (RBF) kernel**: $$
    K(x_i, x_j) = \exp\left(-\gamma ||x_i - x_j||^2 \right)
    $$ Handles complex decision boundaries by mapping data into an n-dimensional space.

RBF is flexible enough that it's used unless the function is kind of obviously linear.

### **7. Hyperparameters in SVM**

-   **Cost parameter** $C$:
    -   **High** $C$ : Small margin, fewer misclassifications (risk of overfitting).
    -   **Low** $C$ : Large margin, allows some misclassifications (better generalization).
-   **Kernel-specific parameters**:
    -   **RBF kernel (**$\gamma$):
        -   **High** $\gamma$ → More flexible decision boundary (risk of overfitting).
        -   **Low** $\gamma$ → Simpler decision boundary (risk of underfitting).
-   **Polynomial Kernel** -\*\*Value of the polynomial expansion ---

## Example 1: Linear Response function

```{r}
library(tidyverse)
library(tidymodels)
library(e1071)
library(kernlab)
library(patchwork)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width = 10, fig.height = 6)
```

# Introduction

This document demonstrates how Support Vector Machines (SVMs) work "under the hood" and illustrates the kernel trick. We'll work with two scenarios:

1.  **Linear Separation**: Data that can be separated by a straight line
2.  **Polynomial Separation**: Data that requires a curved boundary

For the polynomial, we'll fit both a polynomial kernel and an rbf kernel.

# Load Libraries

```{r theme}
# Set theme for consistent plotting
theme_set(theme_minimal(base_size = 12))
```

# Part 1: Linear Separation

## Generate Linearly Separable Data

```{r linear-data}
# Generate linearly separable data with overlap
n <- 200

linear_data <- tibble(
  x1 = rnorm(n, mean = 0, sd = 1),
  x2 = rnorm(n, mean = 0, sd = 1)
) %>%
  mutate(
    # Add substantial noise to create overlap
    decision_value = 0.8 * x1 + 1.2 * x2 - 0.5 + rnorm(n, mean = 0, sd = 1.5),
    class = factor(if_else(decision_value > 0, "A", "B"))
  ) %>%
  select(x1, x2, class)

# Visualize the data
ggplot(linear_data, aes(x = x1, y = x2, color = class)) +
  geom_point(size = 2, alpha = 0.5) +
   geom_abline(slope = -0.8/1.2, intercept = 0.5/1.2, 
              linetype = "dotted", color = "gray30", linewidth = 0.8) +
  scale_color_manual(values = c("A" = "lightblue", "B" = "orange")) +
  labs(title = "Linearly Separable Data with Overlap",
       subtitle = "True boundary: 0.8*x1 + 1.2*x2 = 0.5 (with noise)") +
  coord_fixed()
```

## Split Data

```{r linear-split}
linear_split <- initial_split(linear_data, prop = 0.75, strata = class)
linear_train <- training(linear_split)
linear_test <- testing(linear_split)

cat("Training set:", nrow(linear_train), "observations\n")
cat("Testing set:", nrow(linear_test), "observations\n")
```

```{r hypo-baseline}

linear_train%>%
  group_by(class)%>%
  summarize(prop=n()/nrow(linear_train))

```

## Fit Linear SVM

```{r linear-svm}
# Define model specification
svm_linear_spec <- svm_poly(
  cost = 1,
  degree = 1,  # Linear kernel
  scale_factor = 1
) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

# Create workflow
svm_linear_wf <- workflow() %>%
  add_formula(class ~ x1 + x2) %>%
  add_model(svm_linear_spec)

# Fit the model
svm_linear_fit <- svm_linear_wf %>%
  fit(data = linear_train)

# Extract the fitted model for inspection
svm_linear_model <- extract_fit_engine(svm_linear_fit)
```

## Visualize Linear Decision Boundary

```{r linear-boundary}
# Create prediction grid
grid_linear <- expand_grid(
  x1 = seq(min(linear_data$x1) - 0.5, max(linear_data$x1) + 0.5, length.out = 200),
  x2 = seq(min(linear_data$x2) - 0.5, max(linear_data$x2) + 0.5, length.out = 200)
)

# Get predictions
grid_linear_pred <- grid_linear %>%
  bind_cols(
    predict(svm_linear_fit, new_data = grid_linear, type = "prob")
  ) %>%
  mutate(predicted_class = if_else(.pred_A > 0.5, "A", "B"))

# Plot decision boundary
 ggplot() +
  geom_tile(data = grid_linear_pred, 
            aes(x = x1, y = x2, fill = .pred_A), 
            alpha = 0.3) +
  geom_point(data = linear_train, 
             aes(x = x1, y = x2, color = class), 
             size = 2, alpha = 0.5) +
#     geom_abline(slope = -0.8/1.2, intercept = 0.5/1.2, 
#              linetype = "dotted", color = "gray30", linewidth = 0.8) +
  scale_fill_gradient2(low = "orange", mid = "white", high = "lightblue", 
                       midpoint = 0.5, name = "P(Class A)") +
  scale_color_manual(values = c("A" = "lightblue", "B" = "orange"), 
                     name = "True Class") +
  labs(title = "Linear SVM Decision Boundary",
       subtitle = "Training Data") +
  coord_fixed()
```

## Linear SVM: Test Set Performance

```{r linear-test}
# Predictions on test set
linear_test_pred <- linear_test %>%
  bind_cols(predict(svm_linear_fit, new_data = linear_test)) %>%
  bind_cols(predict(svm_linear_fit, new_data = linear_test, type = "prob"))

# Confusion matrix
conf_mat_linear <- linear_test_pred %>%
  conf_mat(truth = class, estimate = .pred_class)

conf_mat_linear

# Calculate metrics
linear_metrics <- linear_test_pred %>%
  metrics(truth = class, estimate = .pred_class, .pred_A) %>%
  select(-.estimator) %>%
  mutate(across(where(is.numeric), ~round(.x, 3)))

linear_metrics
```

## How Linear Kernel Works

The linear kernel computes similarity between points as a simple dot product:

$$K(x_i, x_j) = x_i^T x_j$$

For our 2D data: $K(x_i, x_j) = x_{i1} \cdot x_{j1} + x_{i2} \cdot x_{j2}$

The decision function is: $$f(x) = \sum_{i=1}^{n} \alpha_i y_i K(x_i, x) + b$$

where $\alpha_i$ are learned weights (support vectors have $\alpha_i > 0$).

```{r linear-kernel-demo}
# Extract support vectors - kernlab stores indices as a matrix
sv_info <- alphaindex(svm_linear_model)
sv_indices <- as.vector(sv_info[[1]])  # Convert to vector
support_vectors <- linear_train %>% slice(sv_indices)

cat("Number of support vectors:", length(sv_indices), "\n")
cat("Percentage of training data:", 
    round(100 * length(sv_indices) / nrow(linear_train), 1), "%\n")

# Visualize support vectors
ggplot() +
  geom_point(data = linear_train, 
             aes(x = x1, y = x2, color = class), 
             size = 1, alpha = 0.5) +
  geom_point(data = support_vectors,
             aes(x = x1, y = x2, color = class),
             size = 3, shape = 1, stroke = 1,alpha=.8) +
  scale_color_manual(values = c("A" = "lightblue", "B" = "orange")) +
  labs(title = "Support Vectors in Linear SVM",
       subtitle = "Circled points are support vectors") +
  coord_fixed()
```

# Part 2: Polynomial Separation

## Generate Polynomial Separable Data

```{r poly-data}
# Generate data with quadratic boundary and overlap
n <- 200

poly_data <- tibble(
  x1 = runif(n, -2, 2),
  x2 = runif(n, -2, 2)
) %>%
  mutate(
    # Quadratic boundary: x2 > x1^2 - 1
    decision_value = x2 - x1^2 + 1 + rnorm(n, mean = 0, sd = 0.4),
    class = factor(if_else(decision_value > 0, "A", "B"))
  ) %>%
  select(x1, x2, class)

# Visualize the data
ggplot(poly_data, aes(x = x1, y = x2, color = class)) +
  geom_point(size = 2, alpha = 0.5) +
    geom_function(fun = function(x) x^2 - 1, 
                linetype = "dotted", color = "gray30", linewidth = 0.8) +
  scale_color_manual(values = c("A" = "lightblue", "B" = "orange")) +
  labs(title = "Polynomial Separable Data with Overlap",
       subtitle = "True boundary: x2 = x1² - 1 (with noise)") +
  coord_fixed()
```

```{r}

poly_train%>%
  group_by(class)%>%
  summarize(prop=n()/nrow(linear_train))
```

## Split Data

```{r poly-split}
poly_split <- initial_split(poly_data, prop = 0.75, strata = class)
poly_train <- training(poly_split)
poly_test <- testing(poly_split)
```

## Fit Polynomial SVM

```{r poly-svm}
# Define polynomial SVM specification
svm_poly_spec <- svm_poly(
  cost = 1,
  degree = 2,  # Polynomial kernel of degree 2
  scale_factor = 1
) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

# Create workflow
svm_poly_wf <- workflow() %>%
  add_formula(class ~ x1 + x2) %>%
  add_model(svm_poly_spec)

# Fit the model
svm_poly_fit <- svm_poly_wf %>%
  fit(data = poly_train)

# Extract fitted model
svm_poly_model <- extract_fit_engine(svm_poly_fit)
```

## Visualize Polynomial Decision Boundary

```{r poly-boundary}
# Create prediction grid
grid_poly <- expand_grid(
  x1 = seq(min(poly_data$x1) - 0.2, max(poly_data$x1) + 0.2, length.out = 200),
  x2 = seq(min(poly_data$x2) - 0.2, max(poly_data$x2) + 0.2, length.out = 200)
)

# Get predictions
grid_poly_pred <- grid_poly %>%
  bind_cols(
    predict(svm_poly_fit, new_data = grid_poly, type = "prob")
  ) %>%
  mutate(predicted_class = if_else(.pred_A > 0.5, "A", "B"))

# Plot decision boundary
 ggplot() +
  geom_tile(data = grid_poly_pred, 
            aes(x = x1, y = x2, fill = .pred_A), 
            alpha = 0.3) +
  geom_point(data = poly_train, 
             aes(x = x1, y = x2, color = class), 
             size = 2, alpha = 0.5) +
  scale_fill_gradient2(low = "orange", mid = "white", high = "lightblue", 
                       midpoint = 0.5, name = "P(Class A)") +
  scale_color_manual(values = c("A" = "lightblue", "B" = "orange"), 
                     name = "True Class") +
  labs(title = "Polynomial SVM Decision Boundary",
       subtitle = "Training Data (degree = 2)") +
  coord_fixed()


```

## Polynomial SVM: Test Set Performance

```{r poly-test}
# Predictions on test set
poly_test_pred <- poly_test %>%
  bind_cols(predict(svm_poly_fit, new_data = poly_test)) %>%
  bind_cols(predict(svm_poly_fit, new_data = poly_test, type = "prob"))

# Confusion matrix
conf_mat_poly <- poly_test_pred %>%
  conf_mat(truth = class, estimate = .pred_class)

print(conf_mat_poly)

# Calculate metrics
poly_metrics <- poly_test_pred %>%
  metrics(truth = class, estimate = .pred_class, .pred_A) %>%
  select(-.estimator) %>%
  mutate(across(where(is.numeric), ~round(.x, 3)))

print(poly_metrics)
```

## How Polynomial Kernel Works

The polynomial kernel of degree $d$ is:

$$K(x_i, x_j) = (x_i^T x_j + c)^d$$

For degree 2: $K(x_i, x_j) = (x_{i1} x_{j1} + x_{i2} x_{j2} + c)^2$

This is equivalent to explicitly transforming the data to higher dimensions, but **without actually computing the transformation**! This is the "kernel trick."

For 2D input with degree 2, the implicit feature space includes: - Original features: $x_1, x_2$ - Squared features: $x_1^2, x_2^2$ - Interaction: $x_1 x_2$ - Plus constant terms

```{r poly-kernel-demo}
# Extract support vectors - kernlab stores indices as a matrix
sv_info_poly <- alphaindex(svm_poly_model)
sv_indices_poly <- as.vector(sv_info_poly[[1]])  # Convert to vector
support_vectors_poly <- poly_train %>% slice(sv_indices_poly)

cat("Number of support vectors:", length(sv_indices_poly), "\n")
cat("Percentage of training data:", 
    round(100 * length(sv_indices_poly) / nrow(poly_train), 1), "%\n")

# Visualize support vectors
ggplot() +
  geom_point(data = poly_train, 
             aes(x = x1, y = x2, color = class), 
             size = 2, alpha = 0.3) +
  geom_point(data = support_vectors_poly,
             aes(x = x1, y = x2, color = class),
             size = 4, shape = 1, stroke = 2) +
  scale_color_manual(values = c("A" = "lightblue", "B" = "orange")) +
  labs(title = "Support Vectors in Polynomial SVM",
       subtitle = "Circled points are support vectors") +
  coord_fixed()
```

# Part 3: RBF (Radial Basis Function) Kernel

The RBF kernel is another popular choice for non-linear problems. It's especially powerful because it can create very flexible decision boundaries.

## Fit RBF SVM on Polynomial Data

```{r rbf-svm}
# Define RBF SVM specification
svm_rbf_spec <- svm_rbf(
  cost = 1,
  rbf_sigma = 0.5  # Controls the "reach" of each support vector
) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

# Create workflow
svm_rbf_wf <- workflow() %>%
  add_formula(class ~ x1 + x2) %>%
  add_model(svm_rbf_spec)

# Fit the model
svm_rbf_fit <- svm_rbf_wf %>%
  fit(data = poly_train)

# Extract fitted model
svm_rbf_model <- extract_fit_engine(svm_rbf_fit)
```

## Visualize RBF Decision Boundary

```{r rbf-boundary}
# Get predictions on grid
grid_rbf_pred <- grid_poly %>%
  bind_cols(
    predict(svm_rbf_fit, new_data = grid_poly, type = "prob")
  ) %>%
  mutate(predicted_class = if_else(.pred_A > 0.5, "A", "B"))

# Plot decision boundary
p_rbf <- ggplot() +
  geom_tile(data = grid_rbf_pred, 
            aes(x = x1, y = x2, fill = .pred_A), 
            alpha = 0.3) +
  geom_point(data = poly_train, 
             aes(x = x1, y = x2, color = class), 
             size = 2, alpha = 0.5) +
  scale_fill_gradient2(low = "orange", mid = "white", high = "lightblue", 
                       midpoint = 0.5, name = "P(Class A)") +
  scale_color_manual(values = c("A" = "lightblue", "B" = "orange"), 
                     name = "True Class") +
  labs(title = "RBF SVM Decision Boundary",
       subtitle = "Training Data") +
  coord_fixed()

print(p_rbf)
```

## RBF SVM: Test Set Performance

```{r rbf-test}
# Predictions on test set
rbf_test_pred <- poly_test %>%
  bind_cols(predict(svm_rbf_fit, new_data = poly_test)) %>%
  bind_cols(predict(svm_rbf_fit, new_data = poly_test, type = "prob"))

# Confusion matrix
conf_mat_rbf <- rbf_test_pred %>%
  conf_mat(truth = class, estimate = .pred_class)

conf_mat_rbf

# Calculate metrics
rbf_metrics <- rbf_test_pred %>%
  metrics(truth = class, estimate = .pred_class, .pred_A) %>%
  select(-.estimator) %>%
  mutate(across(where(is.numeric), ~round(.x, 3)))

rbf_metrics
```

## How RBF Kernel Works

The RBF (Radial Basis Function) kernel, also called the Gaussian kernel, is:

$$K(x_i, x_j) = \exp\left(-\gamma \|x_i - x_j\|^2\right)$$

where $\gamma$ controls the influence of each training example. The kernel measures similarity based on Euclidean distance - points close together have similarity near 1, while distant points have similarity near 0.

For our 2D data: $$K(x_i, x_j) = \exp\left(-\gamma [(x_{i1} - x_{j1})^2 + (x_{i2} - x_{j2})^2]\right)$$

The RBF kernel implicitly maps data to an **infinite-dimensional space** but still computes similarities efficiently. Neat, huh?

```{r rbf-kernel-demo}
# Extract support vectors
sv_info_rbf <- alphaindex(svm_rbf_model)
sv_indices_rbf <- as.vector(sv_info_rbf[[1]])
support_vectors_rbf <- poly_train %>% slice(sv_indices_rbf)

cat("Number of support vectors:", length(sv_indices_rbf), "\n")
cat("Percentage of training data:", 
    round(100 * length(sv_indices_rbf) / nrow(poly_train), 1), "%\n")

# Visualize support vectors
ggplot() +
  geom_point(data = poly_train, 
             aes(x = x1, y = x2, color = class), 
             size = 1, alpha = 0.3) +
  geom_point(data = support_vectors_rbf,
             aes(x = x1, y = x2, color = class),
             size = 4, shape = 1, stroke = 1, alpha=.8) +
  scale_color_manual(values = c("A" = "lightblue", "B" = "orange")) +
  labs(title = "Support Vectors in RBF SVM",
       subtitle = "Circled points are support vectors") +
  coord_fixed()
```

## Compare Polynomial vs RBF on Polynomial Data

```{r compare-poly-rbf}
# Side-by-side comparison
library(patchwork)

p_poly_compare <- ggplot() +
  geom_tile(data = grid_poly_pred, 
            aes(x = x1, y = x2, fill = .pred_A), 
            alpha = 0.3) +
  geom_point(data = poly_train, 
             aes(x = x1, y = x2, color = class), 
             size = 1.5, alpha = 0.5) +
  scale_fill_gradient2(low = "orange", mid = "white", high = "lightblue", 
                       midpoint = 0.5, name = "P(Class A)") +
  scale_color_manual(values = c("A" = "lightblue", "B" = "orange"), 
                     name = "Class") +
  labs(title = "Polynomial Kernel (degree=2)") +
  coord_fixed() +
  theme(legend.position = "none")

p_rbf_compare <- ggplot() +
  geom_tile(data = grid_rbf_pred, 
            aes(x = x1, y = x2, fill = .pred_A), 
            alpha = 0.3) +
  geom_point(data = poly_train, 
             aes(x = x1, y = x2, color = class), 
             size = 1.5, alpha = 0.5) +
  scale_fill_gradient2(low = "orange", mid = "white", high = "lightblue", 
                       midpoint = 0.5, name = "P(Class A)") +
  scale_color_manual(values = c("A" = "lightblue", "B" = "orange"), 
                     name = "Class") +
  labs(title = "RBF Kernel") +
  coord_fixed() +
  theme(legend.position = "none")

print(p_poly_compare + p_rbf_compare)
```

# Comparison: What if we used the wrong kernel?

## Linear SVM on Polynomial Data

```{r wrong-kernel}
# Fit linear SVM to polynomial data
svm_linear_on_poly <- svm_linear_wf %>%
  fit(data = poly_train)

# Create prediction grid
grid_wrong <- grid_poly %>%
  bind_cols(
    predict(svm_linear_on_poly, new_data = grid_poly, type = "prob")
  )

# Plot
p_wrong <- ggplot() +
  geom_tile(data = grid_wrong, 
            aes(x = x1, y = x2, fill = .pred_A), 
            alpha = 0.3) +
  geom_point(data = poly_train, 
             aes(x = x1, y = x2, color = class), 
             size = 2, alpha = 0.5) +
  scale_fill_gradient2(low = "orange", mid = "white", high = "lightblue", 
                       midpoint = 0.5, name = "P(Class A)") +
  scale_color_manual(values = c("A" = "lightblue", "B" = "orange"), 
                     name = "True Class") +
  labs(title = "Linear SVM on Polynomial Data",
       subtitle = "Wrong kernel choice!") +
  coord_fixed()

print(p_wrong)

# Test performance
poly_test_wrong <- poly_test %>%
  bind_cols(predict(svm_linear_on_poly, new_data = poly_test)) %>%
  bind_cols(predict(svm_linear_on_poly, new_data = poly_test, type = "prob"))

wrong_metrics <- poly_test_wrong %>%
  metrics(truth = class, estimate = .pred_class, .pred_A) %>%
  select(-.estimator) %>%
  mutate(across(where(is.numeric), ~round(.x, 3)))

wrong_metrics
```

# Summary Comparison

```{r summary}
# Combine all metrics
all_metrics <- bind_rows(
  linear_metrics %>% mutate(model = "Linear SVM on Linear Data"),
  poly_metrics %>% mutate(model = "Polynomial SVM on Polynomial Data"),
  rbf_metrics %>% mutate(model = "RBF SVM on Polynomial Data"),
  wrong_metrics %>% mutate(model = "Linear SVM on Polynomial Data")
) %>%
  select(model, .metric, .estimate) %>%
  pivot_wider(names_from = .metric, values_from = .estimate)

all_metrics
```

# Key Takeaways

1.  **Linear Kernel**: Works well for linearly separable data. Decision boundary is a hyperplane in the original feature space.

2.  **Polynomial Kernel**: Can capture curved decision boundaries by implicitly mapping data to higher dimensions. Good when you know the relationship is polynomial.

3.  **RBF Kernel**: The most flexible kernel, mapping to infinite dimensions. Can capture very complex, localized decision boundaries. Often a good default choice for non-linear problems.

4.  **The Kernel Trick**: Instead of explicitly transforming data to higher dimensions (expensive!), we compute similarities directly using kernel functions (efficient!).

5.  **Choosing the Right Kernel**: Using a linear kernel on non-linear data leads to poor performance. The kernel choice should match the structure of your data.

6.  **Support Vectors**: Only a subset of training points (support vectors) determine the decision boundary. The model complexity depends on the number of support vectors, not the dimensionality of the feature space.

## Application to Open University Data

A new Dataset!

Today we'll be working with a new dataset, the Open University Dataset. The Open University Learning Analytics Dataset (OULAD), donated to the UCI Machine Learning Repository on December 20, 2015, encompasses data from the years 2013 and 2014. This dataset provides a comprehensive view of student interactions within a virtual learning environment (VLE) and is structured into several interconnected tables, each offering distinct insights:

More about the dataset [here](https://archive.ics.uci.edu/dataset/349/open+university+learning+analytics+dataset).

There are multiple tables that were used to form this dataset

*Student Information (studentInfo.csv)*:

Contains demographic details of 32,593 students, including age, gender, region, highest education level, and disability status. Records the final results of students in their respective module presentations.Lists 22 modules along with their presentations, identified by module and presentation codes. Specifies the length of each module presentation in days.

*Student Registration (studentRegistration.csv)*:

Details the registration dates of students for specific module presentations. Includes unregistration dates for students who withdrew before completion.

*Assessments (assessments.csv)*:

Provides information on 206 assessments across various modules, including assessment types (e.g., Tutor Marked Assessment, Computer Marked Assessment, Final Exam), cut-off dates, and their respective weights.

*Student Assessment (studentAssessment.csv)*:

Records 173,912 entries of student assessment results, capturing submission dates, scores, and indicators of resubmitted assessments.

*Virtual Learning Environment (vle.csv)*:

Catalogs 6,364 VLE materials, such as HTML pages and PDFs, associated with each module. Details the type of activity and the weeks during which the materials are intended to be used.

*Student VLE Interaction (studentVle.csv)*:

Logs 10,655,280 records of student interactions with VLE materials, noting the number of clicks per material per day.

# Data Cleaning

I did the data cleaning for this separately (code is in `_oulad.R`). It begins by loading several datasets related to student performance, assessments, registrations, and virtual learning environment (VLE) interactions. The code then aggregates student interactions with VLE materials, grouping them by module, presentation, and activity type to calculate the total number of clicks per student for each class session.

Next, the script merges the student VLE interaction data with student demographic and performance data. It recodes the final results into two categories: "passed" (including "Pass" and "Distinction") and "not passed" (including "Fail" and "Withdrawn"). The dataset is then transformed into a wider format, where different types of VLE activity become separate columns, making it easier to use for analysis. Finally, missing values represented by "?" are converted to NA, and the cleaned dataset is saved as a CSV file for further use.

```{r load-ou-data}

ou<-read_csv("oulad.csv")%>%
  mutate(result=fct_relevel(as_factor(result),c("passed","not_passed")))%>%
  select(-final_result,-repeatactivity)

```

```{r train-test-split}

ou_split<-initial_split(ou)

ou_train<-training(ou_split)

ou_test<-testing(ou_split)
```

## Baselines!

As always we should take a quick look at the data to examine the baseline rate.

```{r baselines}
ou_train%>%
  group_by(result)%>%
  summarize(proportion = n() / nrow(ou_train)) %>%
  ungroup()

```

So, the baseline to beat will be about 53 percent, the percent of students did not pass their course.

Let's take a quick look at this by a couple of covariates, include the "module" which is the overall course and the "presentation" which is a specific class offering.

```{r conditional-means}

ou_train%>%
  group_by(result)%>%
  count()
 ou_train %>%
  group_by(code_module,code_presentation) %>%
  summarize(
    total = n(),  # Total number in each class group
    passed = sum(result == "passed", na.rm = TRUE),
    proportion_passed = passed / total
  ) %>%
  ungroup()%>%
  arrange(-proportion_passed)%>%
   print(n=100)
```

## Recipe

With those preliminaries out of the way, let's specify our recipe as usual.

```{r formula}
rf_formula<-as.formula("result~.")
```

```{r recipe}
ou_rec<-recipe(rf_formula,ou_train)%>%
  update_role(id_student,new_role="id")%>%
  update_role(result,new_role = "outcome")%>%
  step_unknown(all_nominal_predictors())%>%
  step_other(all_nominal_predictors(),threshold = .05)%>%
  step_dummy(all_nominal_predictors())%>%
  step_zv(all_predictors())%>%
  step_normalize(all_predictors())%>%
  step_impute_mean(all_predictors())
```

Note as always that we must standardize all predictors. There's quite a bit of missing data in this dataset so I'm using impute_mean to preserve existing data in the rows.

## Model Specification

```{r svm-model}
svm_model<-svm_rbf(
  mode="classification",
  cost=tune(),
  rbf_sigma=tune())%>%
  set_engine("kernlab")


```

[kernlab](https://cran.r-project.org/web/packages/kernlab/index.html) is one of the go-to engines for svm. It's a library that includes a large number of kernel-based modeling approaches.

We've got 24,000 units in our testing data, which makes this just big enough for k-fold cross validation.

```{r -resamp}
ou_rs <-vfold_cv(ou_train, v=20)
```

## Tuning Grid

We'll use a latin-square style design this time, which is designed to cover a broader number of possible combinations. The design spreads points throughout the parameter space rather than clustering them. In grid_space_filling(), this is typically implemented using Latin maximin hypercube sampling, which goes further by maximizing the minimum distance between any two points—pushing samples as far apart as possible. The benefit: you get near-grid-search coverage with a fraction of the computational cost, particularly valuable when tuning expensive models.

```{r}
ou_grid<-grid_space_filling(extract_parameter_set_dials(svm_model),size=4)
```

## Workflow

```{r}
ou_wf <- workflow() %>%
  add_model(svm_model) %>%
  add_recipe(ou_rec)
```

## Hyper Parameter Tuning via Cross Validation

```{r}
fit_svm=FALSE

if(fit_svm){
  doParallel::registerDoParallel()
  
ou_tune_results <- tune_grid(
  ou_wf,
  resamples = ou_rs,
  grid = ou_grid,
  metrics = metric_set(accuracy, roc_auc)
)
save(ou_tune_results,file="ou_tune_results.Rdata")
} else {
  load("ou_tune_results.Rdata")
}
```

## Plotting Tuning Results

```{r}
autoplot(ou_tune_results)
```

```{r}
ou_tune_results%>%
  collect_metrics()%>%
  filter(.metric=="roc_auc")%>%
  ggplot(aes(x=rbf_sigma,y=.metric))+
  geom_line()

```

## Selecting Best Parameters

```{r}
best_params <- select_best(ou_tune_results, metric = "roc_auc")
best_params
```

## Final Model

```{r}
final_ou_wf <- finalize_workflow(ou_wf, best_params)
```

The deployed SVM model is a hyperplane whose position and orientation are entirely determined by the support vectors from training. For linear kernels, this compresses to just weights and intercept. For nonlinear kernels, the model retains the support vectors themselves to compute kernel similarities at prediction time

## Fitting Final Model to Training Data

```{r}
final_ou_model <- fit(final_ou_wf, data = ou_train)
```

## Evaluating Model Fit on Test Data

```{r}
# Evaluate on test set
ou_test<-final_ou_model%>%augment(ou_test)

# Compute final accuracy and ROC AUC
final_metrics <- ou_test %>%
  roc_auc(truth = result,.pred_passed) %>%
  bind_rows(
    ou_test %>% accuracy(truth = result, estimate = .pred_class)
  )

# Output results
final_metrics
```

We're able to achieve pretty impressive model fit via SVM, and could likely improve considerably with some more careful selection of hyperparameters.

### **How Similarity is Used to Predict Points in the Testing Dataset**

In **Support Vector Machines (SVMs)**, the **kernel similarity** between a new test point and training points is crucial for making predictions.

------------------------------------------------------------------------

## **1. Prediction in a Linear SVM**

In a **linear kernel**, the decision boundary is a straight line (or hyperplane in higher dimensions), and prediction is based on the **dot product**:

$$
f(x) = \sum_{i \in SV} \alpha_i y_i (x_i \cdot x) + b
$$

where: - $x$ is the test point, - $x_i$ are the **support vectors** (not all training points, only the critical ones), - $y_i$ are the labels of support vectors (+1 or -1), - $\alpha_i$ are learned coefficients (Lagrange multipliers), - $b$ is the bias term.

**Key Idea**: Prediction is a weighted sum of **similarity** (dot product) between the test point and support vectors.

------------------------------------------------------------------------

## **2. Prediction in a Polynomial SVM**

The **polynomial kernel** is defined as:

$$
K(x_i, x_j) = (x_i \cdot x_j + c)^d
$$

where: - $c$ is a constant (we use **1**), - $d$ is the degree of the polynomial (we use **2** for quadratic separation).

#### **Impact on Prediction:**

-   If a test point has **high polynomial similarity** to positive-class support vectors, it is classified as **+1**.
-   If a test point has **higher similarity** to negative-class support vectors, it is classified as **-1**.
-   **Non-linear decision boundaries** emerge because polynomial similarity is not just based on distance but **higher-order relationships**.

------------------------------------------------------------------------

## **3. Prediction in an RBF (Gaussian) SVM**

The **RBF kernel** introduces a different notion of similarity:

$$
K(x_i, x_j) = \exp(-\gamma ||x_i - x_j||^2)
$$

where $\gamma$ controls how **quickly similarity decreases** with distance.

#### **Impact on Prediction:**

-   If a test point is **very close** to a particular support vector, the kernel similarity will be **near 1**.
-   If it is **far away**, the kernel similarity **drops towards 0**.
-   A **weighted sum** of these similarities determines classification:

$$
f(x) = \sum_{i \in SV} \alpha_i y_i K(x_i, x) + b
$$

------------------------------------------------------------------------

## 
