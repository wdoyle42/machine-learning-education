---
title: "Elastic Net with Multinomial Outcomes"
author: "Will Doyle"
date: "2025-01-27"
output: github_document
---

## Multinomial Outcomes and Regularization
 
We can continue to extend our approaches to regularization to models for multinomial outcomes such as multinomial logit. All of the same basic elements will apply, the only difference will be how we think about accuracy and how we calculate the ROC AUC. 

Up to this point, we've been working with binary outcomes—cases where we're predicting one of two possible states. But many real-world prediction problems involve more than two categories. In education policy, for instance, we might want to predict what type of institution a student will attend: a two-year public college, a four-year public university, a four-year private institution, or something else entirely. This is where multinomial models come in.

The good news is that the regularization techniques we've already learned—lasso, ridge, and elastic net—work just as well with multinomial outcomes. The mechanics are nearly identical to what we've been doing, though we'll need to think more carefully about how we measure model performance when we have multiple categories to predict.

```{r}
library(tidyverse)
library(tidymodels)
library(janitor)
```

## Load dataset

Here I'm again going to use sector, but instead of simplifying it to be a binary variable, I'm going to use a set of categories that would be more similar to what we use in policy and practice applications. 

For this example, we'll be working with college sector as our outcome. Rather than collapsing it down to a simple binary variable like "attended college or not," we're going to keep the richer detail that's available in the data. We'll have categories for two-year public institutions, four-year public universities, four-year private colleges, and a couple of other categories. This gives us a more realistic picture of the choices students are actually making.

The data cleaning process here involves collapsing some of the very fine-grained categories in the original data into broader groups that are still meaningful for policy and practice. We're using `fct_collapse` to group similar institution types together—for example, combining doctorate-granting and non-doctorate-granting four-year public institutions into a single "4-year, public" category.

```{r}

hs <- read_csv("hsls_extract3.csv") %>%
  clean_names() %>%
  mutate(across(everything(), ~ ifelse(.  %in% c(-9:-4) , NA, .))) %>%
  mutate(sector = as_factor(x5ps1sec)) %>%
  mutate(
    sector = fct_collapse(
      sector,
      `2-year, other` = c("2-year private, for-profit", "2-year private, nonprofit"),
      `4-year, private` = c(
        "4-year private, nonprofit, doctorate granting",
        "4-year private, nonprofit, nondoctorate granting"
      ),
      `4-year, public` = c(
        "4-year public, doctorate granting",
        "4-year public, nondoctorate granting, primarily baccalaureate"
      ),
      `2-year, public` = c(
        "2-year public",
        "4-year public, nondoctorate granting, primarily subbaccalaureate"
      ),
      other_level = "other"
    )
  ) %>%
  select(-x5ps1sec) %>%
  drop_na()

```

As usual, we'll split our data into training and testing sets. This split ensures that we'll be able to evaluate our model on data it hasn't seen before.

```{r}
hs_split<-initial_split(hs)

hs_train<-training(hs_split)

hs_test<-testing(hs_split)
```


## Understanding Baseline Rate

The baseline rate for multinomial outcomes is just as important as it was for binary outcomes. 

Before we start building any models, it's crucial to understand the distribution of our outcome variable. What proportion of students in our training data attend each type of institution? This baseline rate tells us what we'd predict if we had no information about students at all—we'd just guess the most common category every time.

Understanding these proportions also helps us interpret our model's performance later. If 40% of students attend four-year public universities, and our model correctly predicts this category 60% of the time, that's meaningful improvement over the baseline. But if we're only getting 42% accuracy, we haven't gained much.

```{r}
hs_train%>%
 group_by(sector) %>%
  summarize(proportion = n() / nrow(hs_train)) %>%
  ungroup()
```


## Recipe

The recipe for a multinomial model looks very similar to what we've done before. We're specifying our outcome, handling categorical predictors by creating dummy variables, removing any predictors with zero variance, and normalizing all numeric predictors.

One thing to note: we're using `step_other` with a threshold of .01, which means any category that appears in less than 1% of observations gets collapsed into an "other" category. This helps prevent overfitting to rare categories and keeps our model more generalizable.

```{r}
attend_formula<-as.formula("sector~.")

hs_rec<-recipe(attend_formula,data=hs_train)%>%
  update_role(sector,new_role = "outcome")%>%
  step_other(all_nominal_predictors(),threshold = .01)%>%
  step_dummy(all_nominal_predictors())%>%
  step_zv(all_predictors())%>%
  step_normalize(all_predictors())
```

## Setting Up the Elastic Net Model

Now we get to the elastic net specification. Remember that elastic net combines both lasso (L1) and ridge (L2) regularization by using a `mixture` parameter. When mixture equals 1, we get pure lasso. When it equals 0, we get pure ridge. Values in between give us a blend of the two approaches.

Here we're setting both `penalty` and `mixture` to `tune()`, which means we're going to try out a bunch of different combinations and see which one works best. This is the hyperparameter tuning process we've discussed before.

```{r}
hs_tune_model<- 
  multinom_reg(penalty=tune(),mixture=tune())%>% 
  set_engine("glmnet")
```

The `grid_regular` command creates a grid of parameter values to try. With levels set to 5, we'll get 5 different values for penalty and 5 different values for mixture, giving us 25 total combinations to evaluate. In practice, you might want to try more combinations, but this gives us a good starting point.

```{r}
enet_grid<-grid_regular(extract_parameter_set_dials(hs_tune_model) ,levels=5)
```

## Cross-Validation Setup

We're using Monte Carlo cross-validation with 25 replications. This means we'll randomly split our training data 25 times, fit our model to each split, and average the results. This gives us a robust estimate of how well each combination of hyperparameters performs.

```{r}
hs_rs<-mc_cv(hs_train,times=25)
```

The workflow brings together our model specification and our recipe. 

```{r}
hs_wf<-workflow()%>%
  add_model(hs_tune_model)%>%
  add_recipe(hs_rec)
```

For multinomial outcomes, we care about both accuracy (what proportion of cases did we get right?) and the ROC AUC (how well does the model discriminate between categories overall?). The `metric_set` function lets us specify that we want to evaluate both of these metrics.

```{r}
hs_metrics<-metric_set(accuracy,roc_auc)
```

Now we run the tuning grid across all our resamples. This is where the computational work happens—we're fitting 25 models for each of our 25 hyperparameter combinations, giving us 625 total model fits. The `tune_grid` function handles all of this for us.

```{r}
hs_tune_fit<-hs_wf%>%
  tune_grid(hs_rs,
            grid=enet_grid,
            metrics=hs_metrics)
```


## Accuracy, Sensitivity, Specificity, and AUC

Grabbing the metrics is easy, as usual. 

Once the tuning is complete, we can look at the metrics for each combination of hyperparameters. The output shows us the mean accuracy and mean ROC AUC across all 25 resamples for each penalty/mixture combination.

```{r}
hs_tune_fit%>% collect_metrics()
```


### The ROC AUC for multinomial outcomes

When we discussed the ROC AUC last time, our motivating example was exclusively about binary outcomes. The process for calculating AUC for a multinomial outcome follows the same basic logic, but it's a bit more complicated. We use the Hand/Till method, which involves calculating the AUC for each pairwise comparison across all of the available comparisons and then averaging the results. 

Think about it this way: if we have four categories, we can make six pairwise comparisons (category 1 vs. 2, 1 vs. 3, 1 vs. 4, 2 vs. 3, 2 vs. 4, and 3 vs. 4). For each of these comparisons, we calculate an AUC just like we would for a binary outcome. Then we average all six values to get our overall multinomial AUC.

This approach gives us a single number that summarizes how well our model discriminates between all the categories, not just how often it gets the answer exactly right.

The autoplot shows us visually how different combinations of penalty and mixture affect our ROC AUC. Generally, we're looking for the combination that gives us the highest AUC—that represents the best discrimination between categories.

```{r}
autoplot(hs_tune_fit,metric="roc_auc")
```


## Fitting the model

Let's finalize our workflow and then think about the accuracy and auc of the model when predictions are made in the testing dataset. 

The process for this outcome works just the same as for a binary outcome, we just need to make sure we're using the appropriate metric. 

First we pull the best parameters from the cross-validation.

Once we've identified which hyperparameter combination performed best in our cross-validation, we need to extract those specific values. The `select_best` function does this for us, pulling the penalty and mixture values that gave us the highest ROC AUC.

```{r}
best_params<-select_best(hs_tune_fit,metric="roc_auc")
```

Then we plug those parameters into the workflow.

Now we take those best parameters and plug them back into our workflow. The `finalize_workflow` function updates our workflow with the specific penalty and mixture values we've identified.

```{r}
hs_final<-finalize_workflow(hs_wf,best_params)
```

Then we fit the best fit model to the full training data.

With our hyperparameters locked in, we fit the model to our entire training dataset. Up until now, we've been fitting models to resampled portions of the training data. Now we're using all of it, which gives us the most information possible for our final model.

```{r}
hs_final_fit<-hs_final%>%fit(hs_train)
```


### Variable importance

Glmnet reports coefficients for all of the classes, which is different than many other ways of reporting results—the comparison is always to the null model of intercept only. 

With multinomial models, understanding variable importance gets a bit more complex. Unlike binary outcomes where we have a single set of coefficients, multinomial models give us a separate set of coefficients for each category we're predicting.

The intercepts tell us the baseline log-odds for each category when all predictors are at their mean values. Then we can look at the coefficients for specific categories to see which variables matter most for predicting attendance at, say, two-year public colleges versus four-year private institutions.

Below we're examining the coefficients for two specific categories. For two-year public institutions, we see which variables have the largest (in absolute value) coefficients. The same process for four-year private institutions will show a different pattern—the variables that push students toward four-year private colleges might be quite different from those that push them toward two-year public colleges.

```{r}

hs_final_fit%>%
  extract_fit_parsnip()%>%
  tidy()%>%
  group_by(class)%>%
  filter(term=="(Intercept)")


hs_final_fit%>%
  extract_fit_parsnip()%>%
  tidy()%>%
  filter(class=="2-year, public")%>%
  arrange(-abs(estimate))
  


hs_final_fit%>%
  extract_fit_parsnip()%>%
  tidy()%>%
  filter(class=="4-year, private")%>%
  arrange(-abs(estimate))

 
```

## Evaluating Model Performance

Now comes the moment of truth: how well does our model perform on the testing data? We use `augment` to generate predictions, which gives us not just the predicted category for each student, but also the predicted probabilities for all categories.

```{r}
hs_preds<-augment(hs_final_fit,hs_test)%>%
  mutate(`.pred_class`=as_factor(`.pred_class`))

hs_preds%>%
  select(1:6)%>%
  View()
```

The overall accuracy tells us what proportion of students we correctly classified. Remember to compare this to the baseline rate we looked at earlier—if our accuracy is close to the baseline, we haven't gained much by building a complex model.

```{r}
hs_preds%>%accuracy(estimate = `.pred_class`,truth=sector)
```

It's also worth comparing the distribution of our predictions to the actual distribution of outcomes. Are we over-predicting certain categories and under-predicting others? If so, that might suggest our model has systematic biases we need to address.

```{r}
hs_preds%>%
 group_by(.pred_class) %>%
 rename(Sector=.pred_class)%>%
 summarize(proportion_pred = n() / nrow(hs_preds)) %>%
  full_join(
hs_preds%>%
  group_by(sector)%>%
  rename(Sector=sector)%>%
  summarize(proportion_actual=n()/nrow(hs_preds)))
```

We can also look at accuracy for each predicted category separately. This tells us, for instance: when we predict a student will attend a two-year public college, how often are we right? This category-specific accuracy can reveal patterns that the overall accuracy obscures.

```{r}
hs_preds%>%
  group_by(.pred_class)%>%
  accuracy(estimate=.pred_class,truth=sector)
```

Finally, the ROC AUC on the testing data gives us our overall measure of discrimination. This is the number we optimized during cross-validation, and now we're seeing how well it held up when applied to completely new data.

```{r}
hs_preds%>%select(-`.pred_class`)%>%roc_auc(contains("pred"), truth=sector)
```

### Digression: Multinomial logit and types of constraints

This section gets into some technical details about how multinomial models are parameterized. There are different ways to set up the constraints on the coefficients, and the choice affects how we interpret the results.

The "zero constraint" approach (used by `nnet`) sets one category as the reference and estimates coefficients for all other categories relative to that reference. The "sum to zero constraint" approach (used by `glmnet`) estimates coefficients for all categories such that they sum to zero across categories for each predictor.

Neither approach is inherently better—they're just different ways of representing the same underlying model. But the choice affects how we interpret individual coefficients, so it's worth being aware of which approach your modeling software is using.

```{r}
zero_constraint_mnl<-multinom_reg()%>%
  set_engine("nnet")

sum_to_zero_constraint_mnl<-multinom_reg(penalty=0,mixture=1)%>%
  set_engine("glmnet")%>%
  set_mode("classification")
```

The code below demonstrates the difference between these approaches by fitting both types of models and comparing their predictions. We create a dataset with mean values for all predictors to see what the baseline predictions would be.

```{r}

get_mode <- function(x) {
  # Frequency table of the values
  freq_table <- table(x)
  # Extract the name with the highest frequency
  # (If there's a tie, which.max takes the first occurrence)
  names(freq_table)[which.max(freq_table)]
}

means<-
  hs_train%>%
  summarize(across(where(is.numeric), 
                   ~ mean(.x, na.rm = TRUE)),
            across(where(is.character),
                         ~get_mode(.x)))
```


```{r}
wf<-workflow()%>%
  add_model(zero_constraint_mnl)%>%
  add_recipe(hs_rec)%>%
  fit(hs_train)

result_zero_constraint<-
  wf%>%extract_fit_parsnip()

result_zero_constraint

preds1<-wf%>%augment(means)%>%select(1:6)

```

When we compare predictions from both approaches on the same data, we can see that while the individual predicted probabilities might differ slightly due to the different parameterizations, the rank ordering of probabilities (and thus the final predicted category) should be the same. The sum of predicted probabilities across all categories will always equal 1, regardless of which constraint we use.

```{r}
wf<-workflow()%>%
  add_model(sum_to_zero_constraint_mnl)%>%
  add_recipe(hs_rec)%>%
  fit(hs_train)

result<-wf%>%extract_fit_parsnip()%>%tidy()

preds2<-wf%>%augment(means)%>%select(1:6)

preds1%>%
  rbind(preds2)%>%
  mutate(total=rowSums(across(-.pred_class)))%>%
  View()
```
