---
title: "Classification and Regression Trees (CART)"
author: "Will Doyle"
date: "2025-02-06"
output: github_document
---

Regression trees have an elegant simplicity. The idea is straightforward: for each variable in the dataset, the tree considers a decision rule that splits the data to maximize predictive accuracy. The first split is made using the variable that provides the most information about the outcome, based on a chosen accuracy measure. After the first split, the process continues recursively down each branch, with each subsequent split chosen to maximize the separation of the data according to the accuracy measure.

Three key hyperparameters control the growth of a regression tree. The first is tree depth, which sets the maximum number of splits allowed. The second is minimum sample size per leaf, which ensures that each terminal node contains at least a specified number of observations. The third is cost complexity, which acts as a pruning mechanism.

Cost complexity sets a threshold for how much each additional split must improve model fit to justify its inclusion. A lower cost complexity allows the tree to grow more extensively, capturing more patterns (but potentially overfitting). A higher cost complexity requires each additional split to provide a significant improvement in accuracy, leading to a simpler, more generalizable model.


```{r}
library(tidyverse)
library(tidymodels)
library(janitor)
library(rpart)
library(rpart.plot)
```

```{r}
ou<-read_csv("oulad.csv")%>%
  mutate(result=fct_relevel(as_factor(result),c("passed","not_passed")))%>%
  select(-final_result)
```

```{r}
ou_split<-initial_split(ou)

ou_train<-training(ou_split)

ou_test<-testing(ou_split)
```



```{r}

ou_train%>%
  group_by(result)%>%
  count()
 ou_train %>%
  group_by(code_module,code_presentation) %>%
  summarize(
    total = n(),  # Total number in each class group
    passed = sum(result == "passed", na.rm = TRUE),
    proportion_passed = passed / total
  ) %>%
  ungroup()%>%
  arrange(-proportion_passed)
```

```{r}
cc<-.0001
td<-10
n_min<-10

tree_spec<-decision_tree(
  cost_complexity =cc , # alpha, 0= keep everything, 1=drop everything
  tree_depth = td, # How many splits before coming to a decision
  min_n=n_min)%>% # minimum n for each node
  set_engine("rpart")%>%
  set_mode("classification")
```


```{r}
ou_limited<-ou_train%>%
  select(result,
         num_of_prev_attempts,
         studied_credits)
#,
#        age_band,
#       gender,
#        highest_education)
```

```{r}
tree_formula<-as.formula("result~.")
```


```{r}
tree_rec<-recipe(tree_formula,ou_limited)%>%
  update_role(result,new_role = "outcome")%>%
  step_other(all_nominal_predictors(),threshold = .1)%>%
  step_unknown(all_nominal_predictors())%>%
  step_dummy(all_nominal_predictors())
```

We'll start off by creating a simple tree model and fitting it so we can visualize the results. 

```{r}

cc<-.001
n_min=10
n_trees=10

small_tree <- decision_tree(
  cost_complexity=cc,
  min_n = 10,
  tree_depth=10) %>%
   set_engine("rpart") %>%
   set_mode("classification")

#workflow
 tree_wf <- workflow() %>%
   add_recipe(tree_rec) %>%
   add_model(small_tree) %>%
   fit(ou_limited) #results are found here 

 tree_fit <- tree_wf %>% 
  extract_fit_parsnip()
 
rpart.plot(tree_fit$fit)
 
# Plot tree before pruning
rpart.plot(tree_fit$fit, main = "Initial Tree")

# Display cost complexity table
printcp(tree_fit$fit)

# Prune tree at "optimal" cp
optimal_cp <- tree_fit$fit$cptable[which.min(tree_fit$fit$cptable[, "xerror"]), "CP"]
pruned_tree <- prune(tree_fit$fit, cp = optimal_cp)

# Plot pruned tree
rpart.plot(pruned_tree, main = "Pruned Tree") 
```




```{r}
tree_spec<-decision_tree(
  cost_complexity = tune(), # alpha, 0= keep everything, 1=drop everything
  tree_depth = tune(), # How many splits before coming to a decision
  min_n=tune())%>% # minimum n for each node
  set_engine("rpart")%>%
  set_mode("classification")
```


1. cost_complexity() (Complexity Parameter, cp)

What it does: Prunes the tree by penalizing splits that do not significantly improve model performance.
Higher values: Reduce the number of splits, leading to simpler trees (risking underfitting).
Lower values: Allow more splits, making trees deeper and more complex (risking overfitting).

2. tree_depth() (Maximum Depth of the Tree)
What it does: Limits the number of levels in the tree.
Deeper trees: Capture more complex patterns but risk overfitting.
Shallower trees: Improve generalization but may underfit if too shallow.

3. min_n() (Minimum Number of Observations in a Node)
What it does: Sets the minimum number of data points required in a node for a split to occur.
Higher values: Prevent splitting when there arenâ€™t enough observations, leading to simpler trees.
Lower values: Allow smaller groups to be split, creating more complex trees.

```{r}
tree_grid<-grid_regular(cost_complexity(),
                        tree_depth(),
                        min_n(),
                        levels=4)
```


```{r}
tree_formula<-as.formula("result~.")
```


```{r}
tree_rec<-recipe(tree_formula,ou_train)%>%
  update_role(result,new_role = "outcome")%>%
  update_role(id_student,new_role="id")%>%
  step_other(all_nominal_predictors(),threshold = .1)%>%
  step_unknown(all_nominal_predictors())%>%
  step_dummy(all_nominal_predictors())
```

```{r}
tree_rec%>%prep%>%bake(ou_train)
```

```{r}
ou_fold<-vfold_cv(ou_train,v=20)
```


```{r}
tree_wf<-workflow()%>%
  add_model(tree_spec)%>%
  add_recipe(tree_rec)
```


```{r}

fit_model<-FALSE
if(fit_model){
doParallel::registerDoParallel()

tree_rs <- tree_wf%>%
  tune_grid(
  resamples = ou_fold,
  grid = tree_grid,
  metrics = metric_set(sensitivity,specificity,roc_auc)
)

save(tree_rs,file = "tree_rs.Rdata")
} else {
load("tree_rs.Rdata")
}
```

```{r}
collect_metrics(tree_rs)%>%
  filter(.metric=="roc_auc")%>%
  arrange(-mean)

```

```{r}
autoplot(tree_rs)
```

```{r}
show_best(tree_rs,metric="roc_auc")

final_params<-select_best(tree_rs,metric="roc_auc")
```

```{r}
final_tree<-finalize_model(tree_spec,final_params)
```

```{r}
final_test <- last_fit(final_tree, tree_rec, ou_split)

final_test%>%collect_metrics()
```

```{r}

final_wf<-workflow()%>%
  add_model(final_tree)%>%
  add_recipe(tree_rec)

final_fit <- final_wf%>%fit(ou_train)

```

```{r}

final_fit_result<- final_fit%>% 
  extract_fit_parsnip()

rpart.plot(final_fit_result$fit)

```


