---
title: "Classification and Regression Trees (CART)"
author: "Will Doyle"
date: "2025-02-06"
output: github_document
---

Regression trees have an elegant simplicity. The idea is straightforward: for each variable in the dataset, the tree considers a decision rule that splits the data to maximize predictive accuracy. The first split is made using the variable that provides the most information about the outcome, based on a chosen accuracy measure. After the first split, the process continues recursively down each branch, with each subsequent split chosen to maximize the separation of the data according to the accuracy measure.

Three key hyperparameters control the growth of a regression tree. The first is tree depth, which sets the maximum number of splits allowed. The second is minimum sample size per leaf, which ensures that each terminal node contains at least a specified number of observations. The third is cost complexity, which acts as a pruning mechanism.

Cost complexity sets a threshold for how much each additional split must improve model fit to justify its inclusion. A lower cost complexity allows the tree to grow more extensively, capturing more patterns (but potentially overfitting). A higher cost complexity requires each additional split to provide a significant improvement in accuracy, leading to a simpler, more generalizable model.


```{r}
library(tidyverse)
library(tidymodels)
library(janitor)
library(rpart)
library(rpart.plot)
```

```{r}
ou<-read_csv("oulad.csv")%>%
  mutate(result=fct_relevel(as_factor(result),c("passed","not_passed")))%>%
  select(-final_result)
```


We'll start off by using a very limited version of the dataset. 

```{r}
ou<-ou%>%select(result,
         num_of_prev_attempts,
         studied_credits,
         highest_education)
```


```{r}
ou_split<-initial_split(ou)

ou_train<-training(ou_split)

ou_test<-testing(ou_split)
```

Let's specify a vary basic classification tree. We'll have a low cost complexity, a tree depth of 10, and a minimum node size of 10 units. 


```{r}
tree_formula<-as.formula("result~.")
```


In the recipe, notice that I'm not standardizing. Not needed for trees!
```{r}
tree_rec<-recipe(tree_formula,ou)%>%
  update_role(result,new_role = "outcome")%>%
  step_other(all_nominal_predictors(),threshold = .1)%>%
  step_unknown(all_nominal_predictors())%>%
  step_impute_linear(all_numeric_predictors())%>%
  step_dummy(all_nominal_predictors())
```

We'll start off by creating a simple tree model and fitting it so we can visualize the results. 

```{r}

cc<-.001


small_tree <- decision_tree(
  cost_complexity=cc,
  min_n = 10,
  tree_depth=10) %>%
   set_engine("rpart") %>%
   set_mode("classification")

#workflow
 tree_wf <- workflow() %>%
   add_recipe(tree_rec) %>%
   add_model(small_tree) %>%
   fit(ou_train) #results are found here 
 tree_fit <- tree_wf %>% 
  extract_fit_parsnip()
 
rpart.plot(tree_fit$fit)
 
# Plot tree before pruning
rpart.plot(tree_fit$fit, 
  type = 1,          # show class probabilities at nodes
  extra = 7,        
  under = TRUE,      # put extra info under the box (cleaner)
  faclen = 0,        # don't abbreviate factor level names
  varlen = 0,        # don't abbreviate variable names
  roundint = FALSE,  # don't round split points to integers
  digits = 2         # fewer decimals
)

```

```{r}
# Display cost complexity table
printcp(tree_fit$fit)

# Prune tree at "optimal" cp
optimal_cp <- tree_fit$fit$cptable[which.min(tree_fit$fit$cptable[, "xerror"]), "CP"]
pruned_tree <- prune(tree_fit$fit, cp = optimal_cp)

# Plot pruned tree
rpart.plot(pruned_tree, main = "Pruned Tree",roundint=FALSE) 
```


# Tuning a Classification Tree

Of course, like with everything else you just 
```{r}
tree_spec<-decision_tree(
  cost_complexity = tune(), # alpha, 0= keep everything, 1=drop everything
  tree_depth = tune(), # How many splits before coming to a decision
  min_n=tune())%>% # minimum n for each node
  set_engine("rpart")%>%
  set_mode("classification")
```


1. cost_complexity() (Complexity Parameter, cp)

What it does: Prunes the tree by penalizing splits that do not significantly improve model performance.
Higher values: Reduce the number of splits, leading to simpler trees (risking underfitting).
Lower values: Allow more splits, making trees deeper and more complex (risking overfitting).

2. tree_depth() (Maximum Depth of the Tree)
What it does: Limits the number of levels in the tree.
Deeper trees: Capture more complex patterns but risk overfitting.
Shallower trees: Improve generalization but may underfit if too shallow.

3. min_n() (Minimum Number of Observations in a Node)
What it does: Sets the minimum number of data points required in a node for a split to occur.
Higher values: Prevent splitting when there arenâ€™t enough observations, leading to simpler trees.
Lower values: Allow smaller groups to be split, creating more complex trees.




```{r}
tree_grid<-grid_regular(cost_complexity(),
                        tree_depth(),
                        min_n(),
                        levels=4)
```

```{r}
ou<-read_csv("oulad.csv")%>%
  mutate(result=fct_relevel(as_factor(result),c("passed","not_passed")))%>%
  select(-final_result)
```

```{r}
ou_split<-initial_split(ou)
ou_train<-training(ou_split)
ou_test<-testing(ou_split)
```



```{r}
tree_formula<-as.formula("result~.")
```


```{r}
tree_rec<-recipe(tree_formula,ou_train)%>%
  update_role(result,new_role = "outcome")%>%
  update_role(id_student,new_role="id")%>%
  step_other(all_nominal_predictors(),threshold = .1)%>%
  step_unknown(all_nominal_predictors())%>%
  step_dummy(all_nominal_predictors())
```

```{r}
ou_fold<-vfold_cv(ou_train,v=20)
```


```{r}
tree_wf<-workflow()%>%
  add_model(tree_spec)%>%
  add_recipe(tree_rec)
```


```{r}

fit_model<-FALSE
if(fit_model){
doParallel::registerDoParallel()

tree_rs <- tree_wf%>%
  tune_grid(
  resamples = ou_fold,
  grid = tree_grid,
  metrics = metric_set(sensitivity,specificity,roc_auc)
)

save(tree_rs,file = "tree_rs.Rdata")
} else {
load("tree_rs.Rdata")
}
```

```{r}
collect_metrics(tree_rs)%>%
  filter(.metric=="roc_auc")%>%
  arrange(-mean)

```

```{r}
autoplot(tree_rs)
```

```{r}
show_best(tree_rs,metric="roc_auc")

final_params<-select_best(tree_rs,metric="roc_auc")
```

```{r}
final_tree<-finalize_model(tree_spec,final_params)
```

```{r}
final_test <- last_fit(final_tree, tree_rec, ou_split)

final_test%>%collect_metrics()
```

```{r}

final_wf<-workflow()%>%
  add_model(final_tree)%>%
  add_recipe(tree_rec)

final_fit <- final_wf%>%fit(ou_train)

```

```{r}

final_fit_result<- final_fit%>% 
  extract_fit_parsnip()

## Tee hee
rpart.plot(final_fit_result$fit,roundint = FALSE) 

```

```{r}
ou_test<-final_fit%>%augment(ou_test)

# Compute final accuracy and ROC AUC
final_metrics <- ou_test %>%
  roc_auc(truth = result,.pred_passed) %>%
  bind_rows(
    ou_test %>% accuracy(truth = result, estimate = .pred_class)
  )

# Output results
final_metrics

```

As we'll discuss next time, trees can be *very* prone to overfitting, so it's worth exploring a variety of ways to avoid this. 
