---
title: "Regularization"
author: "Will Doyle"
date: "2025-01-09"
output: pdf_document
---



## Lasso model


One of the key decisions for an analyst is which variables to include. We can make decisions about this using theory, or our understanding of the context, but we can also rely on computational approaches. This is known as *regularization* and it involves downweighting the importance of coefficients from a model based on the contribution that a predictor makes. We're going to make use of a regularization penalty known as the "lasso." The lasso downweights variables mostly be dropping variables that are highly correlated with one another, leaving only one of the correlated variables as contributors to the model. We set the degree to which this penalty will be implemented by setting the "penalty" variable in the model specification. (This is also called the L1 penalty, or L1 regularization)


The lasso model minimizes the RSS plus the penalty times the coefficients, meaning the coefficients have to contribute substantially to minimizing RSS or they'll be quickly downweighted to 0. 

$$ 
=argmin[\sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} x_{ij} \beta_j)^2 + \lambda \sum_{j=1}^{p} |\beta_j|]
$$



## Libraries
```{r}
library(tidyverse)
library(tidymodels)
library(janitor)
```

## Numeric example

We'll start by generating a large dataset with a strong predictor x1, and a weaker predictor, x2

```{r}


# Generate standardized dataset
n <- 1e6  # Number of observations
x1 <- rnorm(n, mean = 0, sd = 1)
x2 <- rnorm(n, mean = 0, sd = 1)

b1=4
b2=.25

y <- (b1 * x1) + (b2 * x2) + rnorm(n, mean = 0, sd = 5)

data <- tibble(x1 = x1, x2 = x2, y = y)
```

We'll then split training and testing as always. 

```{r}
data_split <- initial_split(data, prop = 0.8)
train_data <- training(data_split)
test_data <- testing(data_split)
```

This data barely needs any preprocessing, so the recipe is simple

```{r}
recipe <- recipe(y ~ x1 + x2, data = train_data)
```


Our model specification will be a bit different-- we're going to use the `tune()` function, which implies that we'll use different values for the penalty (lambda in the textbook, also known as l1 regularization). 
```{r}
# Define model specification with tunable penalty
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")
```


Put those together in a workflow. 
```{r}
# Define a workflow
workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(lasso_spec)
```


When we set a tuning value, we're saying that we're going to supply a set of possible values for that parameter. The code below gives a grid with some sensible values using powers of 10 from -3 to 0. Penalty can only be from 0-1, this gives us a good lineup. 
```{r}
# Grid of penalty values
penalty_values <- 10^seq(-3, 0, length.out = 10)
```


The function below uses `map_dfr` to map the penalty values onto the function that fits the data and outputs the results as a data frame, with the columns `predictions`, `rmse_values` and `coef_values`. The resulting data frame is created in the tibble function. The output is then the data frame. 

```{r}

# Fit models for each penalty value and collect results
results <- map_dfr(
  penalty_values,
  function(penalty_value) {
    # Fit the model on training data
    lasso_fit <- linear_reg(penalty = penalty_value, mixture = 1) %>%
      set_engine("glmnet") %>%
      fit(y ~ x1 + x2, data = train_data)

    # Get predictions on the testing data
    predictions <- predict(lasso_fit, test_data) %>%
      bind_cols(test_data)

    # Calculate RMSE
    rmse_value <- rmse(predictions, truth = y, estimate = .pred)

    # Add penalty value and coefficients
    coef_values <- tidy(lasso_fit) %>%
      filter(term != "(Intercept)") %>%
      mutate(penalty = penalty_value)

    tibble(
      penalty = penalty_value,
      rmse = rmse_value$.estimate,
      coef_x1 = coef_values$estimate[coef_values$term == "x1"],
      coef_x2 = coef_values$estimate[coef_values$term == "x2"]
    )
  }
)

```

We can then plot the results. 

```{r}
# Visualize RMSE and coefficients as a function of penalty
results %>%
  pivot_longer(cols = starts_with("coef"), names_to = "term", values_to = "coefficient") %>%
  ggplot(aes(x = penalty, y = coefficient, color = term)) +
  geom_line() +
  scale_x_log10() + ## So the plot is linear
  labs(
    title = "Lasso Regression Coefficients by Penalty",
    x = "Penalty (Log Scale)",
    y = "Coefficient Estimate",
    color = "Predictor"
  ) +
  theme_minimal()

results %>%
  ggplot(aes(x = penalty, y = rmse)) +
  geom_line() +
  scale_x_log10() + 
  labs(
    title = "RMSE by Penalty",
    x = "Penalty (Log Scale)",
    y = "RMSE"
  ) +
  theme_minimal()

```


As you can see, as the penalty increases at first the rmse only decreases a bit. As it goes higher it begins downweighting even the x1 predictor too much and the rmse goes much higher. 

Let's take that and apply it to our high school example, using lasso to predict GPA. 

## Load dataset
```{r}
hs<-read_csv("hsls_extract.csv")%>%clean_names()
```


## Data Cleaning
```{r}
hs <- hs %>%
  mutate(across(-x1txmtscor, ~ ifelse(. < 0, NA, .)))%>%
  drop_na()
```


## Training/Testing


```{r}
hs_split<-initial_split(hs)

hs_train<-training(hs_split)

hs_test<-testing(hs_split)
```


## Recipe
```{r}
hs_formula<-as.formula("x3tgpatot~.")

hs_rec<-recipe(hs_formula,data=hs_train)%>%
  update_role(x3tgpatot,new_role = "outcome")%>%
  step_other(all_nominal_predictors(),threshold = .01)%>%
  step_dummy(all_nominal_predictors())%>%
  step_filter_missing(all_predictors(),threshold = .1)%>%
  step_naomit(all_outcomes(),all_predictors())%>%
  step_corr(all_predictors(),threshold = .95)%>%
  step_zv(all_predictors())%>%
  step_normalize(all_predictors())
```

Now we can update the model to use lasso, which will subset on a smaller number of covariates. In the `tidymodels` setup, ridge is alpha (mixture)=0, while lasso is alpha (mixture)=1.   https://parsnip.tidymodels.org/reference/glmnet-details.htm

We'll set a penalty of .1 just to start. 

```{r}
penalty_spec<-.05

mixture_spec<-1

lasso_fit<- 
  linear_reg(penalty=penalty_spec,
             mixture=mixture_spec) %>% 
  set_engine("glmnet")%>%
  set_mode("regression")
```


## Define the Workflow

```{r}
hs_wf<-workflow()
```

## Add the Model

```{r}
hs_wf<-hs_wf%>%
  add_model(lasso_fit)
```


Now we can add our recipe to the workflow. 

```{r}
hs_wf<-hs_wf%>%
  add_recipe(hs_rec)
```

And fit the data:


```{r}
hs_wf<-hs_wf%>%
  fit(hs_train)
```

Using the same setup as wed did with regression, we'll ad the predictions from the lasso model to the test dataset. 

```{r}
  hs_test<-
  hs_wf%>%
  predict(new_data=hs_test)%>%
  rename(.pred1=.pred)%>%
  bind_cols(hs_test)
```

## Calculate RMSE

Next we can use the `rmse` command to compare the actual outcome in the dataset to the outcome. 

```{r}
hs_test%>%
  rmse(truth=x3tgpatot,estimate=.pred1)
```


We can also look at the coefficients to get a sense of what got included and what got dropped. 
```{r}
hs_wf%>%
  extract_fit_parsnip()%>%
  tidy()%>%
  arrange(-abs(estimate))%>%print(n=50)
```

So, whereas before we had 36 substantive coefficients, now we have four. That's what lasso does, particularly with a high penalty level. 

## Quick Exercise ##

Go back and set the penalty to .01 and rerun the code. What changes in terms of coefficient estimates and rmse?

## Quick Exercise 2

Our first kaggle challenge! Minimize the rmse using L1 regularization and/or feature engineering. 






