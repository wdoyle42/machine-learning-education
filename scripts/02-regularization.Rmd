---
title: "Regularization"
author: "Will Doyle"
date: "2025-01-09"
output: html_document
---



## Lasso model


One of the key decisions for an analyst is which variables to include. We can make decisions about this using theory, or our understanding of the context, but we can also rely on computational approaches. This is known as *regularization* and it involves downweighting the importance of coefficients from a model based on the contribution that a predictor makes. We're going to make use of a regularization penalty known as the "lasso." The lasso downweights variables mostly be dropping variables that are highly correlated with one another, leaving only one of the correlated variables as contributors to the model. We set the degree to which this penalty will be implemented by setting the "penalty" variable in the model specification. (This is also called the L1 penalty, or L1 regularization)


Now we can update the model to use lasso, which will subset on a smaller number of covariates. In the `tidymodels` setup, ridge is alpha (mixture)=0, while lasso is alpha (mixture)=1.   https://parsnip.tidymodels.org/reference/glmnet-details.htm
```{r}
penalty_spec<-.1

mixture_spec<-1

lasso_fit<- 
  linear_reg(penalty=penalty_spec,
             mixture=mixture_spec) %>% 
  set_engine("glmnet")%>%
  set_mode("regression")
```


## Define the Workflow

```{r}
cr_wf<-workflow()
```

## Add the Model

```{r}
cr_wf<-cr_wf%>%
  add_model(lasso_fit)
```


Now we can add our recipe to the workflow. 

```{r}
cr_wf<-cr_wf%>%
  add_recipe(cr_rec)
```

Aad fit the data


```{r}
cr_wf<-cr_wf%>%
  fit(cr_train)
```

