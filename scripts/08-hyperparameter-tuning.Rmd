---
title: "Approaches to Hyperparameter Tuning"
author: "Will Doyle"
date: "2026-02-05"
output: github_document
editor_options: 
  markdown: 
    wrap: sentence
---

```{r}
knitr::opts_chunk$set(cache = TRUE)
```

Today we'll cover two approaches to hyperparameter tuning: simulated annealing and Bayesian optimization.
Both of these are similar in that they sit outside of the modeling process, treating whatever's happening inside the model as a black box and simply looking for combinations of hyperparameters that appear to provide best fit.
The key concept to understand in exploring all of the available hyperparameter space is exploration vs. exploitation.

## **Exploration vs. Exploitation**

### **1. Exploration**

-   **Definition**: Exploring the hyperparameter space broadly to discover promising regions.
-   **Goal**: Avoid getting stuck in **local optima** by considering diverse configurations.
-   **Methods for Encouraging Exploration**:
    -   **Random Search**: Randomly samples hyperparameters instead of following a fixed grid.
    -   **Simulated Annealing (SA)**: Allows worse-performing solutions to be accepted early to escape local minima.
    -   **Bayesian Optimization with High Variance**: Uses models that predict uncertainty to explore uncertain regions.
    -   **Increasing the Search Radius**: Expands the range of hyperparameter changes in iterative methods.

**When to Favor Exploration?** - When little is known about the best hyperparameters.
- When hyperparameter space is **large or complex**.
- When initial tuning attempts show multiple **local optima**.

### **2. Exploitation**

-   **Definition**: Refining hyperparameters in a promising region based on past results.

-   **Goal**: Improve model performance by fine-tuning the best-known configurations.

-   **Methods for Encouraging Exploitation**:

    -   **Grid Search with Small Steps**: Tests small variations around the best-known parameters.

    -   **Bayesian Optimization with Lower Variance**: Prioritizes searching near the best-performing values.

    -   **Simulated Annealing with Low Cooling Coefficient**: Reduces the probability of accepting worse solutions as iterations progress.

**When to Favor Exploitation?**

-   When **early tuning** suggests a strong candidate region.

-   When computational resources are limited, and fine-tuning is more efficient.

-   When the **hyperparameter space is well-behaved** (e.g., smooth and convex).

    -   **Grid Search with Small Steps**: Tests small variations around the best-known parameters.
    
    -   **Bayesian Optimization with Lower Variance**: Prioritizes searching near the best-performing values.
    
    -   **Simulated Annealing with Low Cooling Coefficient**: Reduces the probability of accepting worse solutions as iterations progress.


## **The Tradeoff: Finding the Right Balance**

Hyperparameter tuning must balance **exploration and exploitation** dynamically.
Too much **exploration** wastes computational resources, while too much **exploitation** risks missing the best hyperparameter combinations.

### **Balancing Strategies**

| Strategy | Balances Exploration & Exploitation By... |
|-----------------|-------------------------------------------------------|

| **Simulated Annealing (SA)** | Starts with high exploration (random jumps) and gradually shifts to exploitation as the temperature decreases. |
| **Bayesian Optimization** | Uses a probabilistic model to guide the search, alternating between exploring new areas and refining promising ones. |
| **Genetic Algorithms** | Combines random mutations (exploration) with best-performing selection (exploitation). |


```{r}
library(tidyverse)
library(tidymodels)
library(janitor)
library(finetune)
```

## Setup, Data Cleaning

```{r}
hs<-read_csv("hsls_extract3.csv")%>%
  clean_names()%>%
  mutate(across(everything(), ~ ifelse(.  %in%c(-9:-4) , NA, .)))%>%
  mutate(attend4yr=ifelse(str_detect(x5ps1sec,"4-year"),"attend 4yr","other"))%>%
  mutate(attend4yr=as_factor(attend4yr))%>%
  mutate(attend4yr=fct_relevel(attend4yr,c("attend 4yr","other")))%>%
  select(-x5ps1sec)%>%
  drop_na()
```

## Training/Testing Split

```{r}
hs_split<-initial_split(hs)

hs_train<-training(hs_split)

hs_test<-testing(hs_split)
```

## Recipe

```{r}
attend_formula<-as.formula("attend4yr~.")

hs_rec<-recipe(attend_formula,data=hs_train)%>%
  update_role(attend4yr,new_role = "outcome")%>%
  step_other(all_nominal_predictors(),threshold = .01)%>%
  step_dummy(all_nominal_predictors())%>%
  step_zv(all_predictors())%>%
  step_normalize(all_predictors())
```

## Model

```{r}
hs_tune_model<- 
  logistic_reg(penalty=tune(),mixture=tune())%>% 
  set_engine("glmnet")
```

## Resampling Approach

```{r}
hs_rs<-mc_cv(hs_train,times=25)
```

## Set Metrics

```{r}
hs_metrics<-metric_set(roc_auc)
```

## Compile Workflow

```{r}
hs_wf<-workflow()%>%
  add_model(hs_tune_model)%>%
  add_recipe(hs_rec)
```

# Simulated Annealing for Hyperparameter Tuning via Cross-Validation

## Overview

Simulated Annealing (SA) is a probabilistic optimization algorithm inspired by the annealing process in metallurgy, where a material is slowly cooled to settle into a stable structure.
In the context of hyperparameter tuning, SA systematically explores the hyperparameter space, balancing exploration (searching broadly) and exploitation (refining promising areas) to identify the best-performing configuration.

Unlike grid search or random search, SA dynamically adjusts its search behavior, allowing occasional acceptance of worse solutions early in the process to escape local minima.
Over time, the probability of accepting worse solutions decreases, focusing the search on refining optimal hyperparameters.

## **How Simulated Annealing Works for Hyperparameter Tuning**

Simulated annealing follows these steps when tuning hyperparameters:

1.  **Initialization**: Start with a randomly selected hyperparameter configuration.
2.  **Cross-Validation Evaluation**: Train and evaluate the model using **cross-validation (CV)** to estimate performance.
3.  **Generate New Candidates**:
    -   A new hyperparameter set is **randomly selected** in the **local neighborhood** of the current best solution.
    -   The size of this neighborhood depends on the **radius** (explained below).
4.  **Acceptance Criteria**:
    -   If the new configuration improves performance: Accept it.
    -   If it performs worse: Accept it with some probability determined by the **cooling coefficient**.
5.  **Temperature Reduction**: The acceptance probability **decreases over iterations**, making the search more conservative.
6.  **Stopping or Restarting**:
    -   The search stops if no improvement is seen after a certain number of iterations (**no_improve**).
    -   If the search stagnates, it may **restart** from a previous best-known configuration.

Key Elements in `Tidymodels`

1.  Iterations before stopping: number of iterations without improvement before stopping.

2.  Iterations before restarting: Number of iterations without finding a new best solution before restarting from the previous best.

3.  Radius of local neighborhood: Controls how much the next candidate hyperparameter set differs from the current one.

4.  Cooling coefficient: Controls how quickly the probability of accepting worse solutions decreases over iterations.
    Lower values maintain exploration longer, higher values focus on exploitation.

```{r}

hs_sim_anneal_control<-control_sim_anneal(cooling_coef=.02,
                                          verbose = TRUE,
                                          save_history = TRUE)
                                          
hs_enet_tune_fit <- 
  hs_wf %>%
    tune_sim_anneal(hs_rs,
                    initial = 4,
                    metrics=hs_metrics,
                    control=hs_sim_anneal_control)

```

```{r}
load(file.path(tempdir(), "sa_history.RData"))

result_history


px <- sort(unique(result_history$penalty))
my <- sort(unique(result_history$mixture))

fmt4 <- function(x) format(signif(x, 4), trim = TRUE, scientific = FALSE)


result_history%>%
    mutate(
    penalty_f = factor(penalty, levels = px, labels = fmt4(px)),
    mixture_f = factor(mixture, levels = my, labels = fmt4(my))
  ) %>%
ggplot(aes(x=penalty_f,y=mixture_f,fill=mean))+
  geom_tile()

```


## Exploring Results

```{r}
autoplot(hs_enet_tune_fit)
```

## Select Best Fit Parameters

```{r}
best_params<-select_best(hs_enet_tune_fit,metric="roc_auc")
```

## Finalize Workflow

```{r}
final_enet_workflow <- hs_wf %>%
  finalize_workflow(best_params)
```

## Fit Final Model to Training

```{r}
final_model <- final_enet_workflow %>%
  fit(data = hs_train)
```

## Test Model Fit in Training Data

```{r}
predictions <- final_model %>% augment(hs_test)

predictions%>%roc_auc(`.pred_attend 4yr` ,truth=attend4yr)
```

## Bayesian Optimization

# **Bayesian Optimization for Hyperparameter Tuning**

## **Introduction**

Bayesian Optimization is a **probabilistic model-based approach** for hyperparameter tuning that efficiently balances **exploration** (searching new areas) and **exploitation** (refining the best-known solutions).
Unlike brute-force methods such as grid search or random search, Bayesian Optimization constructs a **surrogate model** (often a Gaussian Process) to predict performance and guide the search intelligently.

This method is particularly useful for **complex, high-dimensional hyperparameter spaces**, where evaluating every possible combination would be computationally expensive.

------------------------------------------------------------------------

## **How Bayesian Optimization Works**

Bayesian Optimization follows these steps:

1.  **Initialize with Random Evaluations**: A few initial hyperparameter sets are randomly selected and evaluated using **cross-validation**.
2.  **Build a Surrogate Model**: A **Gaussian Process (GP)** or another probabilistic model estimates the objective function (e.g., validation accuracy).
3.  **Select Next Hyperparameters Using an Acquisition Function**:
    -   Acquisition functions determine the next hyperparameter set by balancing **exploration vs. exploitation**.
4.  **Evaluate and Update**: The selected hyperparameters are evaluated, and the surrogate model is updated.
5.  **Repeat Until Convergence**: The process continues iteratively, refining the search based on past results.

------------------------------------------------------------------------

## **Acquisition Functions: Choosing the Next Hyperparameters**

The **acquisition function** is crucial in Bayesian Optimization because it determines **how new hyperparameter values are selected**.
It balances **exploration (trying new regions)** and **exploitation (focusing on promising areas).**

### **1. Expected Improvement (EI)**

-   **Formula**:\
    $$
    EI(x) = \mathbb{E} [\max(0, f(x) - f(x^*)) | x]
    $$ where:

    -   $f(x)$ is the predicted performance of a new hyperparameter set.
    -   $f(x^*)$ is the best observed performance so far.

-   **Behavior**:

    -   Prioritizes configurations likely to **outperform** the current best.
    -   Encourages moderate exploration while mostly focusing on **exploitation**.

-   **Use Case**:

    -   When the goal is to **refine existing good hyperparameters** while still allowing some exploration.



------------------------------------------------------------------------

### **2. Probability of Improvement (PI)**

-   **Formula**:\
    $$
    PI(x) = P(f(x) > f(x^*) + \xi)
    $$ where:
    -   $\xi$ is a small positive value ensuring some exploration.
-   **Behavior**:
    -   Focuses on hyperparameter sets that **are most likely to improve performance**.
    -   Can be too **greedy**, favoring exploitation over exploration.
-   **Use Case**:
    -   When rapid improvement is needed, and exploration is less critical.

------------------------------------------------------------------------

### **3. Upper Confidence Bound (UCB)**

-   **Formula**:\
    $$
    UCB(x) = \mu(x) + \kappa \cdot \sigma(x)
    $$ where:
    -   $\mu(x)$ is the predicted mean.
    -   $\sigma(x)$ is the uncertainty estimate.
    -   $\kappa$ controls the **exploration-exploitation tradeoff**.
-   **Behavior**:
    -   **Encourages more exploration** than EI or PI.
    -   Considers both **high predicted performance** and **high uncertainty**.
-   **Use Case**:
    -   When the hyperparameter space is **large and complex**, requiring **broader exploration**.

```{r}
hs_control_bayes<-control_bayes(verbose=TRUE)

hs_enet_tune_fit <- 
  hs_wf %>%
    tune_bayes(hs_rs,
               metrics=hs_metrics,
               objective=exp_improve(),
               control=hs_control_bayes)
```

```{r}
hs_enet_tune_fit%>%collect_metrics()
```

```{r}
autoplot(hs_enet_tune_fit)
```

```{r}
best_params<-select_best(hs_enet_tune_fit,metric="roc_auc")
```

```{r}
final_enet_workflow <- hs_wf %>%
  finalize_workflow(best_params)
```

```{r}
final_model <- final_enet_workflow %>%
  fit(data = hs_train)
```

```{r}
predictions <- final_model %>% augment(hs_test)

predictions%>%roc_auc(`.pred_attend 4yr` ,truth=attend4yr)
```

Notice that with both tuning approaches we're able to substantially improve roc auc over grid search. 
