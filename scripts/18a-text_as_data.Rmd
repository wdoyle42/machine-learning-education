---
title: "Text As Data"
author: "Will Doyle"
date: "2025-04-08"
output: html_document
---

### Cribbed Shamelessly from: https://juliasilge.com/blog/animal-crossing/

```{r}
library(tidyverse)
library(tidymodels)
library(tidytext)
library(textrecipes)
library(vip)

data("stop_words")
```

```{r}
user_reviews <- readr::read_tsv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv")

user_reviews %>%
  count(grade) %>%
  ggplot(aes(grade, n)) +
  geom_col(fill = "midnightblue", alpha = 0.7)
```


```{r}
user_reviews %>%
  filter(grade > 8) %>%
  sample_n(5) %>%
  pull(text)
```

```{r}
user_reviews %>%
  filter(grade > 8) %>%
  sample_n(5) %>%
  pull(text)
```

```{r}
reviews_parsed <- user_reviews %>%
  mutate(text = str_remove(text, "Expand$")) %>%
  mutate(rating = case_when(
    grade > 7 ~ "good",
    TRUE ~ "bad"
  ))
```

```{r}
words_per_review <- reviews_parsed %>%
  unnest_tokens(word, text) %>%
  count(user_name, name = "total_words")

words_per_review %>%
  ggplot(aes(total_words)) +
  geom_histogram(fill = "midnightblue", alpha = 0.8)
```



## Sentiment Analysis Using a Dictionary

We can use a dictionary-based approach to perform sentiment analysis on our text data. Here we use the `bing` lexicon from the `tidytext` package.

```{r}

# Assume 'text_df' is your data frame with a column 'text' containing documents
# Tokenize the text
review_expand <- reviews%>%sample(100)%>%
  unnest_tokens(word, text)

review_expand <- review_expand %>%
  anti_join(stop_words, by = "word")

# Load Bing sentiment lexicon
bing <- get_sentiments("bing")

# Join and calculate sentiment
sentiment_scores <- review_expand %>%
  inner_join(bing, by = "word") %>%
  count(document_id, sentiment) %>%
  tidyr::pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(sentiment_score = positive - negative)

# View sentiment scores
print(sentiment_scores)

# Optional: Plot sentiment
ggplot(sentiment_scores, aes(x = document_id, y = sentiment_score)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Sentiment Score by Document", x = "Document", y = "Sentiment Score")
```

This provides a simple overview of sentiment trends in your text corpus.


```{r}
review_split <- initial_split(reviews_parsed, strata = rating)
review_train <- training(review_split)
review_test <- testing(review_split)
```


```{r}
review_rec <- recipe(rating ~ text, data = review_train) %>%
  step_tokenize(text) %>%
  step_stopwords(text) %>%
  step_tokenfilter(text, max_tokens = 500) %>%
  step_tfidf(text) %>%
  step_normalize(all_predictors())
```

```{r}
review_rec%>%prep(review_train)%>%bake(review_train)
```


```{r}
enet_spec <- logistic_reg(penalty = tune(), mixture=tune()) %>%
  set_engine("glmnet")

enet_wf <- workflow() %>%
  add_recipe(review_rec) %>%
  add_model(enet_spec)
```


```{r}
enet_grid <- grid_regular(extract_parameter_set_dials(enet_spec), levels = 3)
```

```{r}
review_mc<- mc_cv(review_train, strata = rating)
review_mc
```

```{r}
enet_grid <- tune_grid(
  enet_wf,
  resamples = review_mc,
  grid = enet_grid,
  metrics = metric_set(roc_auc, accuracy, sensitivity,specificity)
)
```

```{r}
enet_grid %>%
  collect_metrics()
```

```{r}
enet_grid %>%
  collect_metrics() %>%
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_line(size = 1.5, show.legend = FALSE) +
  facet_wrap(~.metric) +
  scale_x_log10()
```

```{r}
best_auc <- enet_grid %>%
  select_best(metric = "roc_auc")

best_auc
```

```{r}
final_fit <- finalize_workflow(enet_wf, best_auc)

final_fit
```

```{r}
final_fit%>%
  fit(review_train) %>%
  extract_fit_parsnip() %>%
  vi(lambda = best_auc$penalty) %>%
  group_by(Sign) %>%
  top_n(20, wt = abs(Importance)) %>%
  ungroup() %>%
  mutate(
    Importance = abs(Importance),
    Variable = str_remove(Variable, "tfidf_text_"),
    Variable = fct_reorder(Variable, Importance)
  ) %>%
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Sign, scales = "free_y") +
  labs(y = NULL)
```

