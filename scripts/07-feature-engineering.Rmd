---
title: "Adventures in Feature Engineering"
author: "Will Doyle"
date: "2025-01-30"
output: html_document
---

## Feature Engineering

Today we'll be reviewing the kind of amazing set of functions available for feature engineering in tidymodels. We'll focus on a few subsets:

- Transformations
- Categorical/binary variables
- Imputation
- Interactions and Expansions

```{r}
library(tidyverse)
library(tidymodels)
library(janitor)
```


## Transformations


```{r}
hs<-read_csv("hsls_extract.csv")%>%clean_names()
```

```{r}
hs <- hs %>%
  mutate(across(-x1txmtscor, ~ ifelse(. < 0, NA, .)))
```

```{r}
hs_split<-initial_split(hs)

hs_train<-training(hs_split)

hs_test<-testing(hs_split)
```



Transformations refer to any kind of numeric transformation. The key here is linear versus non-linear transforms. Linear transforms just aid in interpretability or ensure identical scales for regularizations. Non-linear transforms are a modeling choice-- they make a fundamental change in how the model works. 

Below, we use step_normalize as a linear transform for all predictors. 

https://www.tidymodels.org/find/recipes/

```{r}
hs_formula<-as.formula("x3tgpatot~.")

hs_rec<-recipe(hs_formula,data=hs_train)%>%
  update_role(x3tgpatot,new_role = "outcome")%>%
  step_other(all_nominal_predictors(),threshold = .01)%>%
  step_dummy(all_nominal_predictors())%>%
  step_zv(all_predictors())%>%
  step_normalize(all_predictors())
```

Let's change math scores to percentiles-- a non linear transform


```{r}
hs_rec<-recipe(hs_formula,data=hs_train)%>%
  update_role(x3tgpatot,new_role = "outcome")%>%
  step_percentile(x1txmtscor)%>%
  step_other(all_nominal_predictors(),threshold = .01)%>%
  step_dummy(all_nominal_predictors())%>%
  step_zv(all_predictors())%>%
  step_normalize(all_predictors())
```


```{r}
hs_tune_model<- 
  linear_reg(penalty=tune(),mixture=tune())%>% 
  set_engine("glmnet")
```

```{r}
enet_grid<-grid_regular(extract_parameter_set_dials(hs_tune_model) ,levels=10)
```

```{r}
hs_rs<-mc_cv(hs_train,times=25,prop=.75) ## More like 1000 in practice
```

Set the workflow, as usual.

```{r}
hs_wf<-workflow()%>%
  add_model(hs_tune_model)%>%
  add_recipe(hs_rec)
```

Then we can use `tune_grid` to run the model through the resampled data, using the grid supplied.

```{r}
hs_enet_tune_fit <- 
  hs_wf %>%
    tune_grid(hs_rs,grid=enet_grid)
```


```{r}
best_params<-select_best(hs_enet_tune_fit,metric="rmse")
```


```{r}
final_enet_workflow <- hs_wf %>%
  finalize_workflow(best_params)
```


```{r}
final_model <- final_enet_workflow %>%
  fit(data = hs_train)
```

```{r}
predictions <- final_model %>% augment(hs_test)
predictions%>%rmse(truth=x3tgpatot,estimate=.pred)
```



We can also log and/or offset

```{r}
hs_rec<-recipe(hs_formula,data=hs_train)%>%
  update_role(x3tgpatot,new_role = "outcome")%>%
  step_log(x1tmtscore)%>%
  step_other(all_nominal_predictors(),threshold = .01)%>%
  step_dummy(all_nominal_predictors())%>%
  step_zv(all_predictors())%>%
  step_normalize(all_predictors())
```


### Categorical and binary variables

A standard here is step_dummy. 
```{r}
hs_rec<-recipe(hs_formula,data=hs_train)%>%
  update_role(x3tgpatot,new_role = "outcome")%>%
  step_log(x1tmtscore)%>%
  step_dummy(all_nominal_predictors())%>%
  step_zv(all_predictors())%>%
  step_normalize(all_predictors())
```


We can combine step other with step dummy, and we may want to vary this threshold. 
```{r}
hs_rec<-recipe(hs_formula,data=hs_train)%>%
  update_role(x3tgpatot,new_role = "outcome")%>%
  step_log(x1tmtscore)%>%
  step_other(all_nominal_predictors(),threshold = .1)%>%
  step_dummy(all_nominal_predictors())%>%
  step_zv(all_predictors())%>%
  step_normalize(all_predictors())
```


Imputation

Imputation in ML is not for inference, but simply to include more data, particularly in interactions. We can use step_
```{r}
hs_rec<-recipe(hs_formula,data=hs_train)%>%
  update_role(x3tgpatot,new_role = "outcome")%>%
  step_log(x1tmtscore)%>%
  step_other(all_nominal_predictors(),threshold = .1)%>%
  step_dummy(all_nominal_predictors())%>%
  step_impute_mean(all_predictors())%>%
  step_zv(all_predictors())%>%
  step_normalize(all_predictors())
```

We can use more sophisticated approaches, including KNN. 

```{r}
hs_rec<-recipe(hs_formula,data=hs_train)%>%
  update_role(x3tgpatot,new_role = "outcome")%>%
  step_log(x1tmtscore)%>%
  step_other(all_nominal_predictors(),threshold = .1)%>%
  step_dummy(all_nominal_predictors())%>%
  step_impute_knn(all_predictors())%>%
  step_zv(all_predictors())%>%
  step_normalize(all_predictors())
```

### Interactions and Expansions

We can interact continuous with continuous
```{r}
hs_rec<-recipe(hs_formula,data=hs_train)%>%
  update_role(x3tgpatot,new_role = "outcome")%>%
  step_log(x1tmtscore)%>%
  step_other(all_nominal_predictors(),threshold = .1)%>%
  step_dummy(all_nominal_predictors())%>%
  step_impute_knn(all_predictors())%>%
  step_zv(all_predictors())%>%
  step_interact(terms=~x1tmtscore:x1schooleng)
  step_normalize(all_predictors())
```

We can interact continuous with with binary
```{r}
hs_rec<-recipe(hs_formula,data=hs_train)%>%
  update_role(x3tgpatot,new_role = "outcome")%>%
  step_other(all_nominal_predictors(),threshold = .1)%>%
  step_dummy(all_nominal_predictors())%>%
  step_impute_knn(all_predictors())%>%
  step_zv(all_predictors())%>%
  step_interact(terms=~x1tmtscore:starts_with("x1paredu"))
  step_normalize(all_predictors())
```

And binary with binary

```{r}
hs_rec<-recipe(hs_formula,data=hs_train)%>%
  update_role(x3tgpatot,new_role = "outcome")%>%
  step_other(all_nominal_predictors(),threshold = .1)%>%
  step_dummy(all_nominal_predictors())%>%
    step_interact(terms=~starts_with("x1locale"):starts_with("x1paredu"))%>%
  step_normalize(all_predictors())
```

Polynomial expansions can be done on any numeric variable, specifying the degreee of the expansion

```{r}
hs_rec<-recipe(hs_formula,data=hs_train)%>%
  update_role(x3tgpatot,new_role = "outcome")%>%
  step_poly(x1txmtscor,degree = 3)%>%
  step_other(all_nominal_predictors(),threshold = .1)%>%
  step_dummy(all_nominal_predictors())%>%
  step_normalize(all_predictors())
```




### In class work



