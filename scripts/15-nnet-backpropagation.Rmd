---
title: "Backpropagation"
author: "Will Doyle"
date: "2025-03-25"
output: html_document
---
## Backpropagation

In this lesson, we'll work through a simulated example to show how backpropagation works. This is somewhat abstracted away from the actual way neural nets function, but close enough to give you a good picture of what's going on.

The function below, `generate_synthetic_data`, creates a synthetic dataset for a multi-class classification problem. It allows the user to specify the number of samples (`n_samples`), the dimensionality of each input vector (`input_dim`), the number of classes (`n_classes`), and the standard deviation of "noise" (`noise_sd`). First, it generates a set of random centroids—one for each class—serving as the mean vectors for the input features. Then, for each sample, it randomly selects a class, retrieves the corresponding centroid, and adds Gaussian noise to generate an input vector centered around that class's centroid. The function stores each input in matrix X and encodes the class labels in a one-hot format in matrix Y. Finally, it returns a list containing X (the feature matrix) and Y (the one-hot encoded label matrix).


```{r}
library(tidyverse)

generate_synthetic_data <- function(n_samples = 1000, 
                                    input_dim = 10, 
                                    n_classes = 5, 
                                    noise_sd = 0.1) {

  
  # Step 1: Create class centroids (each is a 784-dim mean vector)
  centroids <- matrix(rnorm(n_classes * input_dim, mean = 0, sd = 1), 
                      nrow = n_classes, ncol = input_dim)
  
  # Storage for generated data
  X <- matrix(0, nrow = n_samples, ncol = input_dim)
  Y <- matrix(0, nrow = n_samples, ncol = n_classes)  # one-hot encoded
  
  for (i in 1:n_samples) {
    class <- sample(0:(n_classes - 1), 1)  # Random class label 0–9
    centroid <- centroids[class + 1, ]    # Get corresponding centroid
    
    # Generate input vector: add noise around centroid
    x_i <- centroid + rnorm(input_dim, mean = 0, sd = noise_sd)
    
    # Store input and one-hot label
    X[i, ] <- x_i
    Y[i, class + 1] <- 1
  }
  
  list(X = X, Y = Y)
}
```

```{r}
k<-784

n<-1000

j<-10

synth<-generate_synthetic_data(n_samples=n,
                        input_dim=k,
                        n_classes=j,
                        noise_sd = .5)

```

```{r}

X <- synth$X
Y <- synth$Y

labels <- apply(Y, 1, function(row) which(row == 1) - 1)

# Run PCA
pca_result <- prcomp(X, center = TRUE, scale. = TRUE)
pca_df <- as.data.frame(pca_result$x[, 3:4])  # Get first 2 principal components
colnames(pca_df) <- c("PC3", "PC4")
pca_df$class <- as.factor(labels)

# Scatter plot using ggplot2
ggplot(pca_df, aes(x = PC3, y = PC4, color = class)) +
  geom_point(alpha = 0.8, size = 2) +
  labs(
       x = "Principal Component 3",
       y = "Principal Component 4",
       color = "Class") +
  theme_minimal()
```


The `initialize_network` function below sets up a simple two-layer neural network by randomly initializing its weights and biases. This is handled by torch/pytorch internally.   It takes three arguments: `input_dim` (number of input features), `hidden_dim` (number of neurons in the hidden layer), and `output_dim` (number of output classes or units). The function returns a list containing four components: `W1`, a weight matrix connecting the input layer to the hidden layer; `b1`, a bias vector for the hidden layer; `W2`, a weight matrix connecting the hidden layer to the output layer; and `b2`, a bias vector for the output layer. The weights are initialized with small random values uniformly drawn from the interval [-0.1, 0.1], while the biases are initialized to zero. The final line assigns this initialized network to the variable network.
```{r}
initialize_network <- function(input_dim = k, hidden_dim = 5, output_dim = j) {
  list(
    W1 = matrix(runif(hidden_dim * input_dim, -0.1, 0.1), nrow = hidden_dim),
    b1 = rep(0, hidden_dim),
    W2 = matrix(runif(output_dim * hidden_dim, -0.1, 0.1), nrow = output_dim),
    b2 = rep(0, output_dim)
  )
}
network<-initialize_network()
```

```{r}

backprop_step <- function(X, Y, network, lr = 0.1) {
  # INPUTS:
  # x: input vector (numeric vector of length 784 for a 28x28 image)
  # y: one-hot encoded label vector (length 10; e.g., class 3 → c(0,0,1,0,...,0))
  # network: list of current weights and biases
  #   - W1: weight matrix (5 x 784), connects input to hidden layer
  #   - b1: bias vector (5 x 1), for hidden layer
  #   - W2: weight matrix (10 x 5), connects hidden layer to output layer
  #   - b2: bias vector (10 x 1), for output layer
  # lr: learning rate (scalar, step size for updating weights)

  n_samples<-nrow(X)
  
  # --- UNPACK WEIGHTS ---
  W1 <- network$W1  # (5 x 784)
  b1 <- network$b1  # (5 x 1)
  W2 <- network$W2  # (10 x 5)
  b2 <- network$b2  # (10 x 1)

  # --- FORWARD PASS ---

  # Step 1: Compute hidden layer linear combination
  Z1 <- X %*% t(W1) +matrix(rep(b1, each = n_samples), nrow = n_samples)

  
  # Step 2: Apply ReLU activation to hidden layer
  A1 <- pmax(0, Z1)    # a1: (5 x 1) = activated output from hidden layer

  A1<-matrix(A1,n_samples,5)
  
  # Step 3: Compute output layer linear combination
  Z2 <- A1 %*% t(W2) + matrix(rep(b2, each = n_samples), nrow = n_samples)

  # Step 5: Softmax to turn output into probabilities
softmax <- function(z) {
  exp_z <- exp(z - apply(z, 1, max))  # subtract row-wise max for stability
  exp_z / rowSums(exp_z)
}

  Y_hat <- softmax(Z2)  # Predicted probabilities for 10 classes

  # Step 6: Compute cross-entropy loss
  # Loss is a scalar: -log(probability of correct class)
  loss <- -sum(Y * log(Y_hat))  # scalar loss

  # --- BACKWARD PASS (Backpropagation) ---

  # Step 7: Derivative of loss w.r.t. a2 (after softmax)
  dZ2 <- (Y_hat - Y)/n_samples         # (10 x 1): error at output layer

  
  # Gradients for W2 and b2
  dW2 <- t(dZ2) %*% A1            # (10 x 5)
  db2 <- colSums(dZ2)             # (10)
  
  # Step 9: Backpropagate error to hidden layer
  dA1 <- dZ2%*% W2      # (5 x 1): error signal passed backward
  dZ1<-dA1
  # Step 10: ReLU derivative (zero out where z1 was ≤ 0)
  dZ1[Z1 <= 0] <- 0  

  # Step 11: Gradients for W1 and b1
  dW1 <- t(dZ1) %*% X      # (5 x 784): outer product of dz1 and input x
  db1 <- colSums(dZ1)              # (5 x 1): bias gradient is just the error

  # --- UPDATE WEIGHTS USING GRADIENT DESCENT ---

  # Adjust weights and biases in direction that reduces loss
  W1 <- W1 - (lr * dW1)      # (5 x 784): input-to-hidden weights
  b1 <- b1 - (lr * db1)      # (5 x 1): hidden biases
  W2 <- W2 - (lr * dW2)      # (10 x 5): hidden-to-output weights
  b2 <- b2 - (lr * db2)      # (10 x 1): output biases

  # --- RETURN UPDATED MODEL STATE ---
  list(
    network = list(
      W1 = W1,
      b1 = b1,
      W2 = W2,
      b2 = b2
    ),
    prediction = Y_hat,    # softmax probabilities (10 x 1)
    loss = loss            # scalar loss value
  )
}

```


## First Run
```{r}
updated<-backprop_step(X=synth$X,Y=synth$Y,network=network)
```

```{r}
updated$loss
```

## Next Run
```{r}
updated<-backprop_step(X=synth$X,Y=synth$Y,network=updated$network)
```

```{r}
updated$loss
```


```{r}

# Placeholder for loss tracking
loss_history <- numeric(100)

# Run for 100 iterations
for (i in 1:1000) {
  updated <- backprop_step(X = synth$X, Y = synth$Y, network = network, lr = 0.5)
  network <- updated$network  # update network in place
  loss_history[i] <- updated$loss
  
  if (i %% 10 == 0) {
    cat(sprintf("Iteration %d - Loss: %.4f\n", i, updated$loss))
  }
}
```


```{r}
loss_history<-data.frame(loss_history,x=1:1000)
```

```{r}
loss_history%>%
  ggplot(aes(x=x,y=loss_history))+
  geom_line()
```

