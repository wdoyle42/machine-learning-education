---
title: "Backpropagation"
author: "Will Doyle"
date: "2025-03-25"
output: html_document
---

### Simple Backpropagation Example in a Two-Step Process

This example shows how to apply backpropagation in a simple scenario: improving the output of a process that passes through **two sequential transformations**.

---

### âœ¨ Problem Setup

We define a simple model:
- Input `x` goes through two steps:
  1. `z = x * w1`
  2. `y_hat = z + w2`
- Goal: Minimize the squared error loss between `y_hat` and true output `y`:

\[
L = (y_\text{hat} - y)^2
\]

We update `w1` and `w2` using backpropagation and gradient descent.

---

###  tep-by-Step Example in R

```{r}
# Input and target output
y <- 10
x <- 2

# Initial weights
w1 <- 1.0
w2 <- 1.0

# Learning rate
lr <- 0.1

# Forward pass
z <- x * w1         # Step 1
y_hat <- z + w2     # Step 2
loss <- (y_hat - y)^2

# Backward pass
# Derivative of loss w.r.t. w2
# dL/dw2 = 2 * (y_hat - y)
dL_dw2 <- 2 * (y_hat - y)

# Derivative of loss w.r.t. w1
# dL/dw1 = 2 * (y_hat - y) * x
dL_dw1 <- 2 * (y_hat - y) * x

# Update weights
w1 <- w1 - lr * dL_dw1
w2 <- w2 - lr * dL_dw2

# Print updated values
cat("Updated w1:", w1, "\n")
cat("Updated w2:", w2, "\n")
cat("Loss:", loss, "\n")
```

---

### ðŸ‹ï¸ General Function

```{r}
backprop_toy_step <- function(x, y, w1, w2, lr = 0.1) {
  # Forward pass
  z <- x * w1
  y_hat <- z + w2
  loss <- (y_hat - y)^2

  # Gradients
  dL_dw2 <- 2 * (y_hat - y)
  dL_dw1 <- 2 * (y_hat - y) * x

  # Update
  w1 <- w1 - lr * dL_dw1
  w2 <- w2 - lr * dL_dw2

  list(w1 = w1, w2 = w2, loss = loss, y_hat = y_hat)
}
```

---

### Run for Multiple Steps

```{r}
# Initialize
w1 <- 1
w2 <- 1
x <- 2
y <- 10

# Run training loop
for (i in 1:10) {
  result <- backprop_toy_step(x, y, w1, w2, lr = 0.1)
  cat("Iteration", i, "- Loss:", result$loss, "\n")
  w1 <- result$w1
  w2 <- result$w2
}
```





```{r}

  # Input vector (10 features)
  x <- c(0.5, -1.0, 0.3, 0.1, -0.2, 0.8, 0.0, 0.4, -0.5, 1.2)
  
  # True label (one-hot for class 2)
  y <- c(0, 1, 0)  # class 2 is the correct class
  
```


```{r}
initialize_network <- function(input_dim = 10, hidden_dim = 5, output_dim = 3) {
  list(
    W1 = matrix(runif(hidden_dim * input_dim, -0.1, 0.1), nrow = hidden_dim),
    b1 = rep(0, hidden_dim),
    W2 = matrix(runif(output_dim * hidden_dim, -0.1, 0.1), nrow = output_dim),
    b2 = rep(0, output_dim)
  )
}
network<-initialize_network()
```

```{r}
backprop_step <- function(x, y, network) {
  # Unpack weights
  W1 <- network$W1
  b1 <- network$b1
  W2 <- network$W2
  b2 <- network$b2
 
  
  # Forward pass
  z1 <- W1 %*% x + b1
  a1 <- pmax(0, z1)  # ReLU
  
  z2 <- W2 %*% a1 + b2
   a2 <- pmax(0, z2)  # ReLU
  
   softmax <- function(z) {
    exp_z <- exp(z - max(z))
    exp_z / sum(exp_z)
  }
  y_hat <- softmax(a2)
  
  loss <- -sum(y * log(y_hat))
  
  # Backward pass
  dz2 <- y_hat - y
  dW2 <- dz2 %*% t(a1)
  db2 <- dz2
  
  dz1 <- t(W2) %*% dz2
  dz1[z1 <= 0] <- 0  # ReLU derivative
  
  dW1 <- dz1 %*% t(x)
  db1 <- dz1
  
  # Return outputs and gradients
  list(
    prediction = y_hat,
    loss = loss,
    gradients = list(
      dW1 = dW1,
      db1 = db1,
      dW2 = dW2,
      db2 = db2
    )
  )
}

```

