---
title: "Backpropagation"
author: "Will Doyle"
date: "2025-03-25"
output: html_document
---

### Simple Backpropagation Example in a Two-Step Process

This example shows how to apply backpropagation in a simple scenario: improving the output of a process that passes through **two sequential transformations**.

------------------------------------------------------------------------

### âœ¨ Problem Setup

We define a simple model: - Input `x` goes through two steps: 1. `z = x * w1` 2. `y_hat = z + w2` - Goal: Minimize the squared error loss between `y_hat` and true output `y`:

$$
L = (y_\text{hat} - y)^2
$$

We update `w1` and `w2` using backpropagation and gradient descent.

------------------------------------------------------------------------

### Step-by-Step Example in R

```{r}
# Input and target output
y <- 10
x <- 2

# Initial weights
w1 <- 1.0
w2 <- 1.0

# Learning rate
lr <- 0.1

# Forward pass
z <- x * w1         # Step 1
y_hat <- z + w2     # Step 2
loss <- (y_hat - y)^2

# Backward pass
# Derivative of loss w.r.t. w2
# dL/dw2 = 2 * (y_hat - y)
dL_dw2 <- 2 * (y_hat - y)

# Derivative of loss w.r.t. w1
# dL/dw1 = 2 * (y_hat - y) * x
dL_dw1 <- 2 * (y_hat - y) * x

# Update weights
w1 <- w1 - lr * dL_dw1
w2 <- w2 - lr * dL_dw2

# Print updated values
cat("Updated w1:", w1, "\n")
cat("Updated w2:", w2, "\n")
cat("Loss:", loss, "\n")
```

------------------------------------------------------------------------

### ðŸ‹ï¸ General Function

```{r}
backprop_toy_step <- function(x, y, w1, w2, lr = 0.1) {
  # Forward pass
  z <- x * w1
  y_hat <- z + w2
  loss <- (y_hat - y)^2

  # Gradients
  dL_dw2 <- 2 * (y_hat - y)
  dL_dw1 <- 2 * (y_hat - y) * x

  # Update
  w1 <- w1 - lr * dL_dw1
  w2 <- w2 - lr * dL_dw2

  list(w1 = w1, w2 = w2, loss = loss, y_hat = y_hat)
}
```

------------------------------------------------------------------------

### Run for Multiple Steps

```{r}
# Initialize
w1 <- 1
w2 <- 1
x <- 2
y <- 10

# Run training loop
for (i in 1:10) {
  result <- backprop_toy_step(x, y, w1, w2, lr = 0.1)
  cat("Iteration", i, "- Loss:", result$loss, "\n")
  w1 <- result$w1
  w2 <- result$w2
}
```

```{r}

  # Input vector (10 features)
  x <- c(0.5, -1.0, 0.3, 0.1, -0.2, 0.8, 0.0, 0.4, -0.5, 1.2)
  
  # True label (one-hot for class 2)
  y <- c(0, 1, 0)  # class 2 is the correct class
  
```

```{r}
initialize_network <- function(input_dim = 10, hidden_dim = 5, output_dim = 3) {
  list(
    W1 = matrix(runif(hidden_dim * input_dim, -0.1, 0.1), nrow = hidden_dim),
    b1 = rep(0, hidden_dim),
    W2 = matrix(runif(output_dim * hidden_dim, -0.1, 0.1), nrow = output_dim),
    b2 = rep(0, output_dim)
  )
}
network<-initialize_network()
```

```{r}
backprop_step <- function(x, y, network, lr = 0.1) {
  # INPUTS:
  # x: input vector (numeric vector of length 784 for a 28x28 image)
  # y: one-hot encoded label vector (length 10; e.g., class 3 â†’ c(0,0,1,0,...,0))
  # network: list of current weights and biases
  #   - W1: weight matrix (5 x 784), connects input to hidden layer
  #   - b1: bias vector (5 x 1), for hidden layer
  #   - W2: weight matrix (10 x 5), connects hidden layer to output layer
  #   - b2: bias vector (10 x 1), for output layer
  # lr: learning rate (scalar, step size for updating weights)

  # --- UNPACK WEIGHTS ---
  W1 <- network$W1  # (5 x 784)
  b1 <- network$b1  # (5 x 1)
  W2 <- network$W2  # (10 x 5)
  b2 <- network$b2  # (10 x 1)

  # --- FORWARD PASS ---

  # Step 1: Compute hidden layer linear combination
  z1 <- W1 %*% x + b1  # z1: (5 x 1) = weighted input to hidden units

  # Step 2: Apply ReLU activation to hidden layer
  a1 <- pmax(0, z1)    # a1: (5 x 1) = activated output from hidden layer

  # Step 3: Compute output layer linear combination
  z2 <- W2 %*% a1 + b2 # z2: (10 x 1) = weighted input to output layer

  # Step 4: ReLU again (unusual before softmax, but left in per original)
  a2 <- pmax(0, z2)    # a2: (10 x 1)

  # Step 5: Softmax to turn output into probabilities
  softmax <- function(z) {
    exp_z <- exp(z - max(z))      # subtract max for numerical stability
    exp_z / sum(exp_z)            # softmax output (10 x 1)
  }
  y_hat <- softmax(a2)  # Predicted probabilities for 10 classes

  # Step 6: Compute cross-entropy loss
  # Loss is a scalar: -log(probability of correct class)
  loss <- -sum(y * log(y_hat))  # scalar loss

  # --- BACKWARD PASS (Backpropagation) ---

  # Step 7: Derivative of loss w.r.t. a2 (after softmax)
  dz2 <- y_hat - y         # (10 x 1): error at output layer

  # Step 8: Gradients for W2 and b2
  dW2 <- dz2 %*% t(a1)     # (10 x 5): outer product
  db2 <- dz2              # (10 x 1): just the output error

  # Step 9: Backpropagate error to hidden layer
  dz1 <- t(W2) %*% dz2     # (5 x 1): error signal passed backward

  # Step 10: ReLU derivative (zero out where z1 was â‰¤ 0)
  dz1[z1 <= 0] <- 0        # (5 x 1): elementwise

  # Step 11: Gradients for W1 and b1
  dW1 <- dz1 %*% t(x)      # (5 x 784): outer product of dz1 and input x
  db1 <- dz1              # (5 x 1): bias gradient is just the error

  # --- UPDATE WEIGHTS USING GRADIENT DESCENT ---

  # Adjust weights and biases in direction that reduces loss
  W1 <- W1 - lr * dW1      # (5 x 784): input-to-hidden weights
  b1 <- b1 - lr * db1      # (5 x 1): hidden biases
  W2 <- W2 - lr * dW2      # (10 x 5): hidden-to-output weights
  b2 <- b2 - lr * db2      # (10 x 1): output biases

  # --- RETURN UPDATED MODEL STATE ---
  list(
    network = list(
      W1 = W1,
      b1 = b1,
      W2 = W2,
      b2 = b2
    ),
    prediction = y_hat,    # softmax probabilities (10 x 1)
    loss = loss            # scalar loss value
  )
}

network

network<-backprop_step(x=x,y=y,network$network)
```
