---
title: "Elastic Net with Multinomial Outcomes"
author: "Will Doyle"
date: "2025-01-27"
output: github_document
---

## Multinomial Outcomes and Regularization
 
 We can continue to extend our approaches to regularization to models for multinomial outcomes such as
 multinomial logit. All of the same basic elements will apply, the only difference will be how we think about
 accuracy and how we calculate the ROC AUC. 


```{r}
library(tidyverse)
library(tidymodels)
library(janitor)
```

## Load dataset

Here I'm again going to use sector, but instead of simplifying it to be a binary variable, I'm going to use a set of categories that would be more similar to what we use in policy and practice applications. 

```{r}

hs <- read_csv("hsls_extract3.csv") %>%
  clean_names() %>%
  mutate(across(everything(), ~ ifelse(.  %in% c(-9:-4) , NA, .))) %>%
  mutate(sector = as_factor(x5ps1sec)) %>%
  mutate(
    sector = fct_collapse(
      sector,
      `2-year, other` = c("2-year private, for-profit", "2-year private, nonprofit"),
      `4-year, private` = c(
        "4-year private, nonprofit, doctorate granting",
        "4-year private, nonprofit, nondoctorate granting"
      ),
      `4-year, public` = c(
        "4-year public, doctorate granting",
        "4-year public, nondoctorate granting, primarily baccalaureate"
      ),
      `2-year, public` = c(
        "2-year public",
        "4-year public, nondoctorate granting, primarily subbaccalaureate"
      ),
      other_level = "other"
    )
  ) %>%
  select(-x5ps1sec) %>%
  drop_na()

```


```{r}
hs_split<-initial_split(hs)

hs_train<-training(hs_split)

hs_test<-testing(hs_split)
```


## Understanding Baseline Rate

The baseline rate for multinomial outcomes is just as important as it was for binary outcomes. 

```{r}
hs_train%>%
 group_by(sector) %>%
  summarize(proportion = n() / nrow(hs_train)) %>%
  ungroup()



```


## Recipe

```{r}
attend_formula<-as.formula("sector~.")

hs_rec<-recipe(attend_formula,data=hs_train)%>%
  update_role(sector,new_role = "outcome")%>%
  step_other(all_nominal_predictors(),threshold = .01)%>%
  step_dummy(all_nominal_predictors())%>%
  step_zv(all_predictors())%>%
  step_normalize(all_predictors())
```

```{r}
hs_tune_model<- 
  multinom_reg(penalty=tune(),mixture=tune())%>% 
  set_engine("glmnet")
```

```{r}
enet_grid<-grid_regular(extract_parameter_set_dials(hs_tune_model) ,levels=5)
```


```{r}
hs_rs<-mc_cv(hs_train,times=25)
```


```{r}
hs_wf<-workflow()%>%
  add_model(hs_tune_model)%>%
  add_recipe(hs_rec)
```

```{r}
hs_metrics<-metric_set(accuracy,roc_auc)
```


```{r}
hs_tune_fit<-hs_wf%>%
  tune_grid(hs_rs,
            grid=enet_grid,
            metrics=hs_metrics)
```


## Accuracy, Sensitivity, Specificity, and AUC

Grabbing the metrics is easy, as usual. 

```{r}
hs_tune_fit%>% collect_metrics()
```


### The ROC AUC for multinomial outcomes

When we discussed the ROC AUC last time, our motivating example was exclusively about binary outcomes. 
The process for calculating AUC for a multinomial outcome follows the same basic logic, but it's a bit more complicated. We use the Hand/Till method, which involves calculating the AUC for each pairwise comparison across all of the 
available comparisons and then averaging the results. 



```{r}
autoplot(hs_tune_fit,metric="roc_auc")
```


## Fitting the model

Let's finalize our workflow and then think about the accuracy and auc of the model when predictions are made in the testing dataset. 

The process for this outcome works just the same as for a binary outcome, we just need to make sure we're using the appropriate metric. 

First we pull the best parameters from the cross-validation. 
```{r}
best_params<-select_best(hs_tune_fit,metric="roc_auc")
```

Then we plug those parameters into the workflow. 
```{r}
hs_final<-finalize_workflow(hs_wf,best_params)
```

Then we fit the best fit model to the full training data. 
```{r}
hs_final_fit<-hs_final%>%fit(hs_train)
```


### Variable importance

Glmnet reports coefficients for all of the classes,
which is different than many other ways of reporting results, the comparison is always to the null model of intercept only. 

```{r}

hs_final_fit%>%
  extract_fit_parsnip()%>%
  tidy()%>%
  group_by(class)%>%
  filter(term=="(Intercept)")


hs_final_fit%>%
  extract_fit_parsnip()%>%
  tidy()%>%
  filter(class=="2-year, public")%>%
  arrange(-abs(estimate))
  


hs_final_fit%>%
  extract_fit_parsnip()%>%
  tidy()%>%
  filter(class=="4-year, private")%>%
  arrange(-abs(estimate))

 
```


```{r}
hs_preds<-augment(hs_final_fit,hs_test)%>%
  mutate(`.pred_class`=as_factor(`.pred_class`))

hs_preds%>%
  select(1:6)%>%
  View()
```

```{r}
hs_preds%>%accuracy(estimate = `.pred_class`,truth=sector)
```



```{r}
hs_preds%>%
 group_by(.pred_class) %>%
 rename(Sector=.pred_class)%>%
 summarize(proportion_pred = n() / nrow(hs_preds)) %>%
  full_join(
hs_preds%>%
  group_by(sector)%>%
  rename(Sector=sector)%>%
  summarize(proportion_actual=n()/nrow(hs_preds)))
```


```{r}
hs_preds%>%
  group_by(.pred_class)%>%
  accuracy(estimate=.pred_class,truth=sector)
```


```{r}
hs_preds%>%select(-`.pred_class`)%>%roc_auc(contains("pred"), truth=sector)
```

### Digression: Multinomial logit and types of constraints

```{r}
zero_constraint_mnl<-multinom_reg()%>%
  set_engine("nnet")

sum_to_zero_constraint_mnl<-multinom_reg(penalty=0,mixture=1)%>%
  set_engine("glmnet")%>%
  set_mode("classification")
```

```{r}

get_mode <- function(x) {
  # Frequency table of the values
  freq_table <- table(x)
  # Extract the name with the highest frequency
  # (If there's a tie, which.max takes the first occurrence)
  names(freq_table)[which.max(freq_table)]
}

means<-
  hs_train%>%
  summarize(across(where(is.numeric), 
                   ~ mean(.x, na.rm = TRUE)),
            across(where(is.character),
                         ~get_mode(.x)))
```


```{r}
wf<-workflow()%>%
  add_model(zero_constraint_mnl)%>%
  add_recipe(hs_rec)%>%
  fit(hs_train)

result_zero_constraint<-
  wf%>%extract_fit_parsnip()

result_zero_constraint

preds1<-wf%>%augment(means)%>%select(1:6)

```


```{r}
wf<-workflow()%>%
  add_model(sum_to_zero_constraint_mnl)%>%
  add_recipe(hs_rec)%>%
  fit(hs_train)

result<-wf%>%extract_fit_parsnip()%>%tidy()

preds2<-wf%>%augment(means)%>%select(1:6)

preds1%>%
  rbind(preds2)%>%
  mutate(total=rowSums(across(-.pred_class)))%>%
  View()
```
https://stats.stackexchange.com/questions/423054/how-to-interpret-coefficients-of-a-multinomial-elastic-net-glmnet-regression



