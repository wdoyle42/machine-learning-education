---
title: "Validation and Hyperparameter Tuning"
author: "Will Doyle"
date: "2026-01-13"
output: github_document
---

## Cross Validation

The essence of prediction is discovering the extent to which our models can predict outcomes for data that does not come from our sample. Many times this process is temporal. We fit a model to data from one time period, then take predictors from a subsequent time period to come up with a prediction in the future. For instance, we might use data on team performance to predict the likely winners and losers for upcoming soccer games. 

This process does not have to be temporal. We can also have data that is out of sample because it hadn't yet been collected when our first data was collected, or we can also have data that is out of sample because we designated it as out of sample.

The data that is used to generate our predictions is known as 
*training* data. The idea is that this is the data used to train our model, to let it know what the relationship is between our predictors and our outcome. 

That data that is used to validate our predictions is known as *testing* data. With testing data, we take our trained model and see how good it is at predicting outcomes using out of sample data. 

One very simple approach to this would be to cut our data in two parts. This is what we've done so far.  We could then train our model on one part of the data, then test it on the other part. This would tell us whether our measure of model fit (e.g. rmse) is similar or different when we apply our model to out of sample data. 

But this would only be a "one-shot" approach. It would be better to do this multiple times, cutting the data into two parts, then fitting the model to the training data, and then checking its predictions against the validation data set. That way, we could generate a large number of measures of accuracy to see how well the model fits on lots of different possible out-of-sample predictions. 

This process is called *cross validation*, and it involves two important decisions: first, how will the data be cut, and second,how many times will the validation run?
 
## Monte Carlo Resampling

Monte Carlo resampling is a method used in the context of cross-validation to evaluate machine learning models. Monte Carlo resampling involves randomly splitting the data multiple times into *analysis* (training) and *assessment* (testing) sets, ensuring that the evaluation of model fit in the testing dataset isn't unduly affected by a single "draw" of the testing data. 

We'll get everything set up for modeling in the hsls data before structuring the data for resampling. 

```{r}
library(tidyverse)
library(tidymodels)
library(janitor)
library(vip)
```


## Load dataset
```{r}
hs<-read_csv("hsls_extract.csv")%>%clean_names()
```


## Data Cleaning
```{r}
hs <- hs %>%
 mutate(across(where(is.numeric) ,
                ~ replace(.x, .x %in% -9:-1, NA))) %>%
  drop_na()
```

```{r}
hs_split<-initial_split(hs)

hs_train<-training(hs_split)

hs_test<-testing(hs_split)
```


## Recipe
```{r}
hs_formula<-as.formula("x3tgpatot~.")

hs_rec<-recipe(hs_formula,data=hs_train)%>%
  update_role(x3tgpatot,new_role = "outcome")%>%
  step_other(all_nominal_predictors(),threshold = .01)%>%
  step_dummy(all_nominal_predictors())%>%
  step_filter_missing(all_predictors(),threshold = .1)%>%
  step_naomit(all_outcomes(),all_predictors())%>%
  step_zv(all_predictors())%>%
  step_normalize(all_predictors())
```

We have two values that we need to specify to set up a lasso model. The `penalty` value sets the $\lambda$ from the above notes. The `mixture` value sets the proportion ridge (L2) or lasso (L1). Setting mixture to 1 makes the model pure lasso. 

```{r}
penalty_spec<-.1

mixture_spec<-1

lasso_fit<- 
  linear_reg(penalty=penalty_spec,
             mixture=mixture_spec) %>% 
  set_engine("glmnet")%>%
  set_mode("regression")
```


The code below will generate a resampled dataset using Monte Carlo resampling. The default is to split it 75/25, with 25 replications.

```{r}
hs_rs<-mc_cv(hs_train,times=25,prop=.75) ## More like 1000 in practice
```


Set the workflow, as usual. 
```{r}
hs_wf<-workflow()%>%
  add_model(lasso_fit)%>%
  add_recipe(hs_rec)
```


We can then fit the model to the resampled data via `fit_resamples`.
```{r}
hs_lasso_fit<-hs_wf%>%
  fit_resamples(hs_rs)
```

The model has now been fit to 25 versions of the analysis data. Let's look at the metrics using `collect_metrics`. This will take the coefficients from the analysis datasets and generate predictions from the assessment datasets. 
```{r}
hs_lasso_fit%>%collect_metrics()
```


We can also pull certain metrics like rmse one at a time if we want.
```{r}
hs_lasso_fit%>%
  unnest(.metrics)%>%
  filter(.metric=="rmse")%>%
  ggplot(aes(x=.estimate))+
  geom_density()
```


## Model Tuning

The problem with the above is that I arbitrarily set the value of penalty to be .1. Do I know this was correct? No! What we need to do is try out a bunch of different values of the penalty, and see which one gives us the best model fit. This process has the impressive name of "hyperparameter tuning via cross validation" but it could just as easily be called "trying a bunch of stuff to see what works."

Below I'm going to give the argument `tune()` for the value of penalty. This will allow us to "fill in" values later. 

Of course we don't know what penalty to use in the lasso model, so we can tune it. This is set up
by using the `penalty=tune()` approach below. 

```{r}
hs_tune_model<- 
  linear_reg(penalty=tune(),mixture=mixture_spec)%>% 
  set_engine("glmnet")
```

Now that we've said which parameter to tune, we'll use the `grid_regular` command to get a set of nicely spaced out values. This command is specific to the hyperparameter, so it will choose reasonable values for the penalty hyperparameter. 

```{r}
lasso_grid<-grid_regular(extract_parameter_set_dials(hs_tune_model), levels=10)
```

We can use `update_model` to change our workflow with the new model. 
```{r}
hs_wf<-hs_wf%>%
  update_model(hs_tune_model)
```

Then we can use `tune_grid` to run the model through the resampled data, using the grid supplied. 
```{r}
hs_lasso_tune_fit <- 
  hs_wf %>%
    tune_grid(hs_rs,grid=lasso_grid)
```

## Examine Results

Lets' take a look and see which models fit better. 

```{r}
hs_lasso_tune_fit%>%
  collect_metrics()%>%
  filter(.metric=="rmse")%>%
  arrange(mean)
```

## Completing the workflow

Once we've identified the hyperparameter that has the lowest mean and SE in the rmse, the next step is to finalize the workflow using that hyperparameter. 

First, we'll pull the best parameters. 
```{r}
best_params <- hs_lasso_tune_fit %>%
  select_best(metric = "rmse") 
```

Next, we'll finalize the workflow using those parameters.
```{r}
final_lasso_workflow <- hs_wf %>%
  finalize_workflow(best_params)
```

And then we'll fit the model to the full training dataset.
```{r}
final_model <- final_lasso_workflow %>%
  fit(data = hs_train) 
```

```{r}
predictions <-augment(final_model,new_data=hs_test)
```

```{r}
predictions%>%  
metrics(truth = x3tgpatot, estimate = .pred)
```

```{r}
final_model%>%
  extract_fit_parsnip()%>%
  tidy()%>%
  filter(abs(estimate)>0)%>%
  print(n=nrow(.))
```

```{r}
final_model%>%
  extract_fit_parsnip()%>%
  vip(num_features=20)
```

This represents a completed workflow-- the parameters are set, and the model is deployable.  

## K-fold Cross Validation

K-fold cross-validation is a widely used method to evaluate and tune machine learning models by splitting the dataset into 
k equally sized subsets (folds). It systematically rotates the roles of training and testing datasets to provide robust performance estimates.

How It Works:
First, we partition the data:

The dataset is randomly divided into k equal (or nearly equal) folds.
Each fold serves as the testing dataset once, while the remaining 
k−1 folds are combined to form the training dataset.

Model Training and Evaluation:

For each fold:
- The model is trained on the aggregated k−1 folds (*analysis* (training) set).
- The model is tested on the left-out fold (*assessment* (testing) set).
- The performance metric (e.g., RMSE, accuracy, is calculated on the assessment set.
- This process is repeated k times (once for each fold).

Aggregating Results:

The evaluation metrics from all k iterations are averaged to produce a single performance estimate.
This reduces the risk of biased evaluations due to a single train-test split.

To structure the data for k-fold resampling, we use the `vfold_cv` command. Here we'll do 20 folds:
```{r}
hs_vf<-vfold_cv(hs_train,v = 20)
```


And then we can add the new tuning grid to our workflow. 
```{r}
hs_lasso_tune_fit <- 
  hs_wf %>%
    tune_grid(hs_vf,grid=lasso_grid)
```

## Examine Results

Lets' take a look and see which models fit better using the k-fold approach. 

```{r}
hs_lasso_tune_fit%>%
  collect_metrics()%>%
  filter(.metric=="rmse")%>%
  arrange(mean)
```


## Ridge Regression

Ridge regression is another technique for regularization, which helps to solve the multicollinearity problem by adding a degree of bias to the regression estimates. It includes every predictor in the model but shrinks the coefficients of less important predictors towards zero, thus minimizing their impact on the model.

In the tidymodels setup, ridge is specified with mixture=0 and penalty set to the desired value. A value of mixture = 1 corresponds to a pure lasso model, while mixture = 0 indicates ridge regression. Values of mixture in between will set the balance between the two in the elastic net model, which we'll cover shortly.

The code below runs through what should be a familiar set of steps to tune a ridge regression. 

```{r}
mixture_spec=0
hs_tune_model<- 
  linear_reg(penalty=tune(),mixture=mixture_spec)%>% 
  set_engine("glmnet")
```

```{r}
ridge_grid<-grid_regular(extract_parameter_set_dials(hs_tune_model) ,levels=10)
```

```{r}
hs_wf<-hs_wf%>%
  update_model(hs_tune_model)
```

```{r}
hs_ridge_tune_fit <- 
  hs_wf %>%
    tune_grid(hs_rs,grid=ridge_grid)
```

We examine the rmse from the cross validated ridge regression here. 
```{r}
hs_ridge_tune_fit%>%
  collect_metrics()%>%
  filter(.metric=="rmse")%>%
  arrange(mean)
```

And as always, select the best model fit. 
```{r}
best_ridge_penalty <- hs_lasso_tune_fit %>%
  select_best(metric = "rmse") 

final_ridge_workflow <- hs_wf %>%
  finalize_workflow(best_ridge_penalty)


final_ridge_model <- final_ridge_workflow %>%
  fit(data = hs_train) 

```

Examining the coefficients for a ridge regression shows how they're more likely to be "downweighted" as opposed to set to 0 as in lasso, particularly in this case where very low values for the penalty are favored. 

```{r}
final_ridge_model%>%
  extract_fit_parsnip()%>%
  tidy()%>%
  filter(abs(estimate)>0)%>%
  print(n=nrow(.))
```

```{r}
final_ridge_model%>%
  extract_fit_parsnip()%>%
  vip(num_features=20)
```

We can then see how the ridge model did relative to the testing dataset. 

```{r}
ridge_predict<-augment(final_ridge_model,new_data=hs_test)

predictions%>%  
metrics(truth = x3tgpatot, estimate = .pred)
```

The next step will be to combine ridge and lasso, in what's called an elastic net approach. 


