---
title: "Elastic Net with Binary Outcomes"
author: "Will Doyle"
date: "2025-01-23"
output: pdf_document
---

## Binary Outcomes and Regularization
 
Regularization can you be used in a wide variety of applications, including for
binary outcomes. We'll make sure to specify that we're using a model
for classification instead of a model for regression, and then use a loss function
appropriate for a binary outcome-- generally the area under the receiver/operator curve, or AUC.

```{r}
library(tidyverse)
library(tidymodels)
library(janitor)
library(vip)
```

## Load dataset

```{r}
hs<-read_csv("hsls_extract3.csv")%>%
  clean_names()%>%
  mutate(across(everything(), ~ ifelse(.  %in%c(-9:-4) , NA, .)))%>%
  mutate(attend4yr=ifelse(str_detect(x5ps1sec,"4-year"),"attend 4yr","other"))%>%
  mutate(attend4yr=as_factor(attend4yr))%>%
  mutate(attend4yr=fct_relevel(attend4yr))%>%
  select(-x5ps1sec)%>%
  drop_na()
```


```{r}
hs_split<-initial_split(hs)

hs_train<-training(hs_split)

hs_test<-testing(hs_split)
```


## Understanding Baseline Rate

The absolute first thing we want to do is to understand the baseline rate: what proportion of the training
dataset is 0 or 1. 

```{r}
hs_train%>%
 group_by(attend4yr) %>%
  summarize(proportion = n() / nrow(hs_train)) %>%
  ungroup()
```

So, we'll need to keep in mind for everything else that the proportion of the traning dataset that attends a four-year is .36, and the proportion that does not is .64. 

Any model MUST beat this performance. In this case, I can guarantee 64 percent accuracy simply by assuming no one in the testing dataset goes to college. We want any modeling process to beat this baseline rate-- by a lot. This problem gets harder as the proportions go to 1 or 0.

# Checking the baseline rate against other factors

We can then begin to examine the baseline rate across levels of other factors. Below, I calculate the proportion of the training data that attends a four-year institution by family income. 

```{r}
 hs_train %>%
  group_by(x1famincome) %>%
  summarize(
    total = n(),  # Total number in each income group
    attending_4yr = sum(attend4yr == "attend 4yr", na.rm = TRUE),
    proportion_4yr = attending_4yr / total
  ) %>%
  ungroup()%>%
  arrange(-proportion_4yr)
```

The relationship is plotted below. 

```{r}
 hs_train %>%
  group_by(x1famincome) %>%
  summarize(
    total = n(),  # Total number in each income group
    attending_4yr = sum(attend4yr == "attend 4yr", na.rm = TRUE),
    `P(Four-Year)` = attending_4yr / total
  ) %>%
  ungroup()%>%
  mutate(x1famincome=fct_reorder(x1famincome,`P(Four-Year)`))%>%
  rename(`Family Income Level`=x1famincome)%>%
  ggplot(aes(x=`Family Income Level`,y=`P(Four-Year)`,fill=`Family Income Level`))+
  geom_col()+
  coord_flip()+
  theme_minimal()+
  theme(legend.position="none")
```


```{r}
hs_train %>%
  group_by(x1famincome) %>%
  summarize(
    total = n(),
    p = mean(attend4yr == "attend 4yr", na.rm = TRUE),
    .groups = "drop"
  ) %>%
  summarize(
    overall_accuracy = weighted.mean(pmax(p, 1 - p), w = total)
  )
```



Just using this kind of conditional mean approach shows the substantial variation in four-year attendance by income. As we add in more features we *should* be able to improve performance over the baseline and the conditional mean approach. 

## Recipe

We'll set the recipe the normal way. 

```{r}
attend_formula<-as.formula("attend4yr~.")

hs_rec<-recipe(attend_formula,data=hs_train)%>%
  update_role(attend4yr,new_role = "outcome")%>%
  step_other(all_nominal_predictors(),threshold = .01)%>%
  step_dummy(all_nominal_predictors())%>%
  step_zv(all_predictors())%>%
  step_normalize(all_predictors())
```

Our model will now be a logistic regression. We'll set up an elastic net approach, tuning both the penalty and the mixture. 

```{r}
hs_tune_model<- 
  logistic_reg(penalty=tune(),mixture=tune())%>% 
  set_engine("glmnet")
```

Tuning grid looks as before. 

```{r}
enet_grid<-grid_regular(extract_parameter_set_dials(hs_tune_model) ,levels=5)
```

And we'll do our normal small version of monte carlo resampling. 
```{r}
hs_rs<-mc_cv(hs_train,times=25)
```


We'll combine everything into our workflow as per our normal approach.
```{r}
hs_wf<-workflow()%>%
  add_model(hs_tune_model)%>%
  add_recipe(hs_rec)
```

I want four metrics back to understand what's happening: 

- Sensitivity (sens): what proportion of positive outcomes are correctly predicted?
- Specificity (spec): what proportion of negative outcomes are correctly predicted?
- Accuracy (accuracy): what's the overall proportion of correct predictions? 
- Area Under the Receiver/Operator Curve (roc_auc): more on this later
```{r}
hs_metrics<-metric_set(sens,spec,accuracy,roc_auc)
```


Now we'll run the model on the resampled datasets. 
```{r}
hs_tune_fit<-hs_wf%>%
  tune_grid(hs_rs,
            grid=enet_grid,
            metrics=hs_metrics)
```


## Accuracy, Sensitivity, Specificity, and AUC

We can start by examining all of the metrics across the results. 
```{r}
hs_tune_fit%>% collect_metrics()
```


### Threshold for prediction

Logistic regression doesn't output classes. Instead it outputs the probability of a case being in each class. Since the outcome is binary, we can just use the probability of being in the "positive" outcome, or attended college. To turn the result into a classification, we need to set a threshold above which we will decide that the case will be classified as a positive outcome. By default, this is always set to .5, but there's nothing that says that we need to stick with that assumption. More on this later. 

### Accuracy

Accuracy is just the percent of cases correctly predicted. For this measure to be
realistically evaluated, it must be compared with the baseline rate. 

```{r}
autoplot(hs_tune_fit,metric="accuracy")
```

In this case, did we achieve model fits that "beat" the baseline. 

### Sensitivity

Sensitivity is the probability that a positive outcome is correctly predicted. For our example, this means accurately predicting that someone goes to college. Of course, this can quite easily be achieved by simply predicting all 1s.

```{r}
autoplot(hs_tune_fit,metric="sens")
```

### Specificity

Specificity is the probability that a negative outcome is correctly predicted. For our example, this means accurately predicting that someone does NOT go to college (outcome "other"). Sensitivity can be maximized by simply predicting. 

```{r}
autoplot(hs_tune_fit,metric="spec")
```

### What's a "good" model?

This can depend on context. For rare outcomes, accuracy may be enough. In some situations, a model with high sensitivity or specificity may be more desirable. For most cases, we'd like a model where sensitivity and specificity remain high regardless of threshold used. Enter the AUC!

### The ROC AUC

The receiver/operator characteristic area under the curve (ROC AUC) combines sensitivity and specificity at every possible value of the threshold. AUC goes from .5 (random prediction) to 1 (perfect, sensitivity and specificity are both 1 at every value of the threshold). We tend to think of AUC in terms of academic grades-- below .7 is not so good, .7 to .8 is OK, .8 to .9 is pretty good, and above .9 is very good model performance. As with every other prediction problem this is very context-dependent, but that gives you a good start. 


```{r}
autoplot(hs_tune_fit,metric="roc_auc")
```


## Fitting the model

Let's finalize our workflow and then think about the sensitivity, specificity and auc of the model when predictions are made in the testing dataset. 

The process for this outcome works just the same as for a continuous outcome, we just need to make sure we're using the appropriate metric. 

First we pull the best parameters from the cross-validation. 
```{r}
best_params<-select_best(hs_tune_fit,metric="roc_auc")
```

Then we plug those parameters into the workflow. 
```{r}
hs_final<-finalize_workflow(hs_wf,best_params)
```

Then we fit the best fit model to the full training data. 
```{r}
hs_final_fit<-hs_final%>%fit(hs_train)
```

### Variable importance
```{r}
hs_final_fit%>%
  extract_fit_parsnip()%>%
  tidy()%>%
  arrange(-estimate)
```

Finally, we can make predictions in the testing dataset. I'm using `augment` here, which automates the steps we were previously doing by hand, of adding predictions to the testing data. Note that there are actually three different predictions added, the predicted class, the probability of attendance, and the probability of another outcome. 

```{r}
hs_preds<-augment(hs_final_fit,hs_test)%>%
  mutate(across(.cols=c(attend4yr,`.pred_class`),~as_factor(.)))%>%
  mutate(attend4yr=fct_relevel(attend4yr, "attend 4yr","other"))%>%
  mutate(`.pred_class`=fct_relevel(`.pred_class`, "attend 4yr","other"))
```

```{r}
accuracy(hs_preds,truth=attend4yr,estimate=.pred_class)

sensitivity(hs_preds,truth=attend4yr,estimate=.pred_class)

specificity(hs_preds,truth=attend4yr,estimate=.pred_class)
```


Now let's see how sensitivity and specificity vary as a function of different thresholds-- this is what's meant by a receiver/operator characteristic curve. 

```{r}

threshold_results <- hs_preds %>%
  mutate(attend4yr=as_factor(attend4yr))%>%
  roc_curve(
    truth = attend4yr,
    `.pred_attend 4yr`
  )

```

This shows how the sensitivity and specificity vary as a function of different thresholds. At very low values of the threshold, we're likely to get the most predicted positive outcomes, and therefore high sensitivity. At very high values of the threshold, we're likely to get more predicted negative outcomes, and therefore high specificity. The area under the curve defined by sensitivity on the y axis and 1-specificity on the x axis is the auc. 

The graphs below plot sensitivity and specificity as a function of the threshold used (in reverse order) then shows the auc. 

```{r}

library(patchwork)

p1<-threshold_results%>%
  pivot_longer(-.threshold,names_to="type")%>%
  mutate(type=fct_relevel(type,"specificity","sensitivity"))%>%
  ggplot(aes(x=.threshold,y=value,color=type))+
  geom_line()+
  scale_x_reverse()+
  theme_minimal()
  
  
p2<-autoplot(threshold_results)

p1+p2
```

```{r}
roc_auc(hs_preds,truth=attend4yr,`.pred_attend 4yr`)
```

The bottom line is for most binary classification tasks we're going to be using AUC, but don't forget about accuracy, sensitivity and specificity, particularly when evaluating the model itself. 


