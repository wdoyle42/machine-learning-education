---
title: "Trivia Night!"
author: "Will Doyle"
date: "2025-03-06"
output: github_document
---



### **Easy Level**
1. **What is the purpose of regularization in a GLM model?**  
   a) To make the model more complex  
   b) To prevent overfitting by penalizing large coefficients  
   c) To remove outliers from the dataset  
   d) To improve accuracy by using all available features  


2. **Which tree-based model is an ensemble method that builds multiple decision trees and averages their predictions?**  


3. **Which type of validation method divides the dataset into multiple subsets, training the model on most of the subsets while testing on the remaining one, repeating this process multiple times so that each subset is used for testing exactly once?**  


### **Medium Level**
4. **What is the main advantage of XGBoost over traditional decision trees?**  
   a) It requires no hyperparameter tuning  
   b) It uses boosting to iteratively improve weak models  
   c) It only works on small datasets  
   d) It does not use feature importance  

  

5. **Which of the following is a key assumption when using Support Vector Machines (SVMs) with a linear kernel?**  

   a) The data must be normally distributed  
   b) The classes must be linearly separable  
   c) There must be at least 1000 observations  
   d) The features must be standardized  


6. **Which two hyperparameters are commonly tuned in elastic net regression using cross-validation?**  


 
---

### **Hard Level**
7. **In Bayesian optimization for hyperparameter tuning, what role does the acquisition function play?**  
   a) It selects the next set of hyperparameters to evaluate  
   b) It initializes the first batch of hyperparameters randomly  
   c) It penalizes high-dimensional search spaces  
   d) It directly minimizes the loss function  

 
8. **What is one potential drawback of using simulated annealing for hyperparameter tuning?**  
   a) It always converges to the same optimal solution  
   b) It cannot escape local minima  
   c) It requires another hyperparameter-- the  predefined schedule for temperature reduction  
   d) It is only applicable to linear regression models  

 
9. **In glmnet, what happens to the number of nonzero coefficients as the lambda parameter increases? Does the number increase or decrease?**




Bonus question plus bet:

**Which learning method we discussed builds learners sequentially, where each new learner corrects the errors of the previous ones?**

